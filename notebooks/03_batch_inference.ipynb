{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Setup + Config\n",
    "# ============================================================\n",
    "import json, re, torch, gc, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# ‚îÄ‚îÄ UPDATE THESE PATHS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ADAPTER_DIR  = '/kaggle/input/medgemma-loopguard-v2/transformers/default/1'\n",
    "PATIENT_FILE = '/kaggle/input/loopguard-patients/patients.json'\n",
    "BASE_MODEL   = 'google/medgemma-1.5-4b-it'\n",
    "OUTPUT_FILE  = '/kaggle/working/patients_with_ai_final.json'\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "BNB_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    ")\n",
    "\n",
    "print('‚úÖ Config ready')\n",
    "print(f'   VRAM available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated())/1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Load Model + Adapter\n",
    "# ============================================================\n",
    "print('Loading base MedGemma...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "print('Loading LoopGuard v2 adapter...')\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
    "model.eval()\n",
    "\n",
    "print(f'‚úÖ Model ready | VRAM used: {torch.cuda.memory_allocated()/1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Inference + Parsing Functions\n",
    "#\n",
    "# Key lessons applied from eval phase:\n",
    "#   1. decode_output: split on '<start_of_turn>model' to preserve\n",
    "#      the PRIMARY HYPOTHESIS label (eval showed it was being truncated)\n",
    "#   2. parse_fields: lookahead to next field boundary avoids\n",
    "#      bleeding content between fields\n",
    "#   3. Urgency normalization: 'moderate' -> 'medium', default 'medium'\n",
    "#   4. clinical_note is a dict ‚Äî always use ['text'] key\n",
    "# ============================================================\n",
    "\n",
    "FIELD_PATTERNS = [\n",
    "    ('primary_hypothesis',      r'PRIMARY HYPOTHESIS:'),\n",
    "    ('differential_diagnoses',  r'DIFFERENTIAL DIAGNOSES:'),\n",
    "    ('key_supporting_evidence', r'KEY SUPPORTING EVIDENCE:'),\n",
    "    ('urgency_level',           r'URGENCY LEVEL:'),\n",
    "    ('tests_ordered',           r'TESTS ORDERED:'),\n",
    "    ('clinical_reasoning',      r'CLINICAL REASONING:'),\n",
    "]\n",
    "\n",
    "FIELD_LABEL_RE = (\n",
    "    r'(?:PRIMARY HYPOTHESIS|DIFFERENTIAL DIAGNOSES|KEY SUPPORTING EVIDENCE|'\n",
    "    r'URGENCY LEVEL|TESTS ORDERED|CLINICAL REASONING):'\n",
    ")\n",
    "\n",
    "\n",
    "def decode_output(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract model-generated text from full decoded string.\n",
    "    MedGemma chat format: prompt ends with '<start_of_turn>model\\n'.\n",
    "    Take everything AFTER the last occurrence of that marker.\n",
    "    This ensures PRIMARY HYPOTHESIS: label is fully preserved.\n",
    "    \"\"\"\n",
    "    marker = '<start_of_turn>model'\n",
    "    if marker in raw:\n",
    "        return raw.split(marker)[-1].lstrip('\\n').strip()\n",
    "    # Should not happen with MedGemma chat format, but safe fallback\n",
    "    return raw.strip()\n",
    "\n",
    "\n",
    "def parse_fields(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract all 6 structured fields from generated text.\n",
    "    Uses lookahead to next field label as the boundary.\n",
    "    \"\"\"\n",
    "    # Strip markdown bold that some outputs include\n",
    "    clean = re.sub(r'\\*\\*', '', text)\n",
    "    result = {}\n",
    "\n",
    "    for field_key, pattern in FIELD_PATTERNS:\n",
    "        m = re.search(pattern, clean, re.IGNORECASE)\n",
    "        if m:\n",
    "            start = m.end()\n",
    "            # Find where next field starts\n",
    "            next_m = re.search(FIELD_LABEL_RE, clean[start:], re.IGNORECASE)\n",
    "            end = start + next_m.start() if next_m else start + 800\n",
    "            value = clean[start:end].strip().strip('[]')\n",
    "            result[field_key] = value\n",
    "        else:\n",
    "            result[field_key] = ''\n",
    "\n",
    "    # Normalize urgency\n",
    "    u = result.get('urgency_level', '').lower()\n",
    "    if 'high' in u:\n",
    "        result['urgency_level'] = 'high'\n",
    "    elif 'low' in u:\n",
    "        result['urgency_level'] = 'low'\n",
    "    elif 'medium' in u or 'moderate' in u:\n",
    "        result['urgency_level'] = 'medium'\n",
    "    else:\n",
    "        result['urgency_level'] = 'medium'\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_inference(note_text: str) -> tuple:\n",
    "    \"\"\"Run LoopGuard v2 on clinical note text. Returns (parsed_dict, raw_generated).\"\"\"\n",
    "    prompt = (\n",
    "        '<start_of_turn>user\\n'\n",
    "        'Extract diagnostic information from this clinical note.\\n\\n'\n",
    "        f'Clinical Note:\\n{note_text}\\n\\n'\n",
    "        'Output ONLY these 6 fields:\\n'\n",
    "        'PRIMARY HYPOTHESIS: [main diagnosis]\\n'\n",
    "        'DIFFERENTIAL DIAGNOSES: [comma-separated alternatives]\\n'\n",
    "        'KEY SUPPORTING EVIDENCE: [comma-separated findings]\\n'\n",
    "        'URGENCY LEVEL: [high/medium/low]\\n'\n",
    "        'TESTS ORDERED: [comma-separated tests]\\n'\n",
    "        'CLINICAL REASONING: [brief explanation]'\n",
    "        '<end_of_turn>\\n<start_of_turn>model\\n'\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors='pt',\n",
    "        truncation=True, max_length=1024\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=600,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    raw = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    generated = decode_output(raw)\n",
    "    parsed = parse_fields(generated)\n",
    "    return parsed, generated\n",
    "\n",
    "\n",
    "print('‚úÖ Functions ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Load Patients + Smoke Test on P001\n",
    "# Verify decode and parse are correct before full batch\n",
    "# ============================================================\n",
    "with open(PATIENT_FILE) as f:\n",
    "    patients = json.load(f)['patient_scenarios']\n",
    "\n",
    "print(f'‚úÖ Loaded {len(patients)} patients')\n",
    "\n",
    "# Smoke test\n",
    "test = patients[0]\n",
    "note_text = test['clinical_note']['text']  # clinical_note is a dict\n",
    "\n",
    "print(f'\\nüî¨ Smoke test: {test[\"patient_id\"]} ({test[\"ground_truth_diagnosis\"]})')\n",
    "parsed, raw = run_inference(note_text)\n",
    "\n",
    "print(f'\\n--- RAW OUTPUT (first 400 chars) ---')\n",
    "print(raw[:400])\n",
    "print(f'\\n--- PARSED ---')\n",
    "for k, v in parsed.items():\n",
    "    status = '‚úÖ' if v else '‚ùå MISSING'\n",
    "    print(f'  {status} {k}: {v[:80] if v else \"\"}')\n",
    "\n",
    "fields_ok = sum(1 for v in parsed.values() if v)\n",
    "print(f'\\n  {fields_ok}/6 fields | urgency={parsed[\"urgency_level\"]}')\n",
    "print('\\n‚úÖ Smoke test passed ‚Äî proceed to full batch' if fields_ok >= 5 else '\\n‚ö†Ô∏è  Check output before proceeding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Full Batch ‚Äî All 20 Patients\n",
    "# ============================================================\n",
    "results = []\n",
    "field_keys = [k for k, _ in FIELD_PATTERNS]\n",
    "\n",
    "print('Running LoopGuard v2 on all 20 patients...\\n')\n",
    "\n",
    "for i, patient in enumerate(patients):\n",
    "    pid = patient['patient_id']\n",
    "    note_text = patient['clinical_note']['text']\n",
    "\n",
    "    print(f'[{i+1:02d}/20] {pid}...', end=' ', flush=True)\n",
    "\n",
    "    parsed, raw = run_inference(note_text)\n",
    "\n",
    "    fields_present = sum(1 for v in parsed.values() if v)\n",
    "    urgency = parsed['urgency_level']\n",
    "\n",
    "    tick = '‚úÖ' if fields_present == 6 else '‚ö†Ô∏è'\n",
    "    print(f'{tick} {fields_present}/6 | urgency={urgency}')\n",
    "\n",
    "    # Preserve all original patient data, add ai_analysis block\n",
    "    enriched = {\n",
    "        **patient,\n",
    "        'ai_analysis': {\n",
    "            **parsed,\n",
    "            '_raw_output': raw,\n",
    "            '_fields_present': fields_present,\n",
    "        }\n",
    "    }\n",
    "    results.append(enriched)\n",
    "\n",
    "print(f'\\n‚úÖ Done: {len(results)}/20 patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: QC Report + Save\n",
    "# ============================================================\n",
    "print('=== BATCH QC REPORT ===')\n",
    "print(f'{\"Patient\":<8} {\"Fields\":<8} {\"Urgency\":<10} Missing')\n",
    "print('-' * 55)\n",
    "\n",
    "complete_count = 0\n",
    "urgency_dist = {'high': 0, 'medium': 0, 'low': 0}\n",
    "missing_tally = {k: 0 for k in field_keys}\n",
    "\n",
    "for r in results:\n",
    "    ai = r['ai_analysis']\n",
    "    fp = ai['_fields_present']\n",
    "    urg = ai['urgency_level']\n",
    "    missing = [k for k in field_keys if not ai.get(k)]\n",
    "\n",
    "    if fp == 6:\n",
    "        complete_count += 1\n",
    "    urgency_dist[urg] = urgency_dist.get(urg, 0) + 1\n",
    "    for k in missing:\n",
    "        missing_tally[k] += 1\n",
    "\n",
    "    tick = '‚úÖ' if fp == 6 else '‚ö†Ô∏è'\n",
    "    print(f\"{r['patient_id']:<8} {tick}{fp}/6   {urg:<10} {missing or 'none'}\")\n",
    "\n",
    "print()\n",
    "print(f'Complete (6/6):     {complete_count}/20')\n",
    "print(f'Urgency dist:       {urgency_dist}')\n",
    "print(f'Missing field tally:{[(k,v) for k,v in missing_tally.items() if v > 0]}')\n",
    "\n",
    "# Save\n",
    "output = {'patient_scenarios': results}\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "size_kb = len(json.dumps(output)) / 1024\n",
    "print(f'\\n‚úÖ Saved ‚Üí {OUTPUT_FILE} ({size_kb:.1f} KB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Spot Check ‚Äî 3 Patients Full Output\n",
    "# P001 (ovarian cancer, high), P009 (hyponatremia, high), P019 (thyroid ca, low)\n",
    "# Manually verify raw output and parsing look correct\n",
    "# ============================================================\n",
    "spot_ids = ['P001', 'P009', 'P019']\n",
    "\n",
    "for r in results:\n",
    "    if r['patient_id'] not in spot_ids:\n",
    "        continue\n",
    "    ai = r['ai_analysis']\n",
    "    print(f\"\\n{'='*65}\")\n",
    "    print(f\"  {r['patient_id']} | GT: {r['ground_truth_diagnosis']}\")\n",
    "    print(f\"{'='*65}\")\n",
    "    print(f\"RAW (first 500 chars):\\n{ai['_raw_output'][:500]}\")\n",
    "    print(f\"\\nPARSED:\")\n",
    "    for k in field_keys:\n",
    "        val = ai.get(k, '')\n",
    "        icon = '‚úÖ' if val else '‚ùå'\n",
    "        print(f\"  {icon} {k}: {val[:100] if val else 'EMPTY'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
