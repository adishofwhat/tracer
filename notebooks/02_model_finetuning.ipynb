{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# MedGemma Fine-Tuning v2 ‚Äî LoopGuard Hypothesis Extraction\n\n**Model:** `google/medgemma-1.5-4b-it`  \n**Method:** LoRA (r=16, alpha=32) + 4-bit quantization  \n**Data:** `training_final_400.json` (421 examples, flat array format)  \n**Expected runtime:** ~3‚Äì4 hours on T4  \n\n---\n\n## Workflow\n1. **Cell 1** ‚Äî Install dependencies ‚Üí restart kernel\n2. **Cells 2‚Äì3** ‚Äî Imports + data loading\n3. ‚ö†Ô∏è **Cell 4** ‚Äî DATA INSPECTION ‚Üí **run and share output before continuing**\n4. **Cell 5** ‚Äî Load model & tokenizer\n5. **Cell 6** ‚Äî HuggingFace login (needed for MedGemma gated access)\n6. **Cell 7** ‚Äî Tokenize with prompt masking fix\n7. ‚ö†Ô∏è **Cell 8** ‚Äî TOKEN LENGTH CHECK ‚Üí **run and share output before continuing**\n8. **Cell 9** ‚Äî LoRA config\n9. **Cell 10** ‚Äî Training args\n10. **Cell 11** ‚Äî Trainer setup\n11. **Cell 12** ‚Äî üöÄ TRAIN (3‚Äì4 hours)\n12. **Cell 13** ‚Äî Save model\n13. **Cell 14** ‚Äî Test inference\n14. **Cell 15** ‚Äî Training report",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 1: Install Dependencies\n# Run once, then restart kernel before continuing\n# ============================================================\nprint(\"üì¶ Installing dependencies...\")\n\n!pip uninstall -y -q transformers peft trl bitsandbytes accelerate\n!pip install -q transformers>=4.47.0\n!pip install -q peft>=0.13.0\n!pip install -q trl>=0.11.0\n!pip install -q accelerate>=0.34.0\n!pip install -q bitsandbytes>=0.46.1\n\nprint(\"\\n‚úÖ Done! ‚ö†Ô∏è  RESTART KERNEL NOW before running any other cells.\")\nprint(\"   Kernel ‚Üí Restart ‚Üí then start from Cell 2\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 2: Imports\n# ============================================================\nimport torch\nimport json\nimport os\nfrom datetime import datetime\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    BitsAndBytesConfig,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom trl import SFTTrainer\nfrom datasets import Dataset\n\nprint(\"‚úÖ Imports successful\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 3: Load Data\n# File: training_final_400.json (flat array format)\n# ============================================================\nDATA_FILENAME = \"training_final_400.json\"\n\n# Search for the file in common Kaggle input locations\ndef find_data_file(filename):\n    search_roots = [\"/kaggle/input\", \"/kaggle/working\"]\n    for root in search_roots:\n        for dirpath, _, filenames in os.walk(root):\n            if filename in filenames:\n                return os.path.join(dirpath, filename)\n    return None\n\ndata_path = find_data_file(DATA_FILENAME)\nassert data_path is not None, f\"‚ùå Could not find '{DATA_FILENAME}' in /kaggle/input. Make sure you added the dataset.\"\nprint(f\"üìÇ Found data at: {data_path}\")\n\nwith open(data_path, 'r') as f:\n    raw_data = json.load(f)\n\n# Handle both flat array and wrapped format\nif isinstance(raw_data, list):\n    examples = raw_data\nelif isinstance(raw_data, dict) and 'examples' in raw_data:\n    examples = raw_data['examples']\nelse:\n    raise ValueError(\"‚ùå Unexpected data format. Expected a list or dict with 'examples' key.\")\n\nprint(f\"‚úÖ Loaded {len(examples)} examples\")\nprint(f\"   Keys in first example: {list(examples[0].keys())}\")\nprint(f\"   Output fields: {list(examples[0]['output'].keys())}\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 4: DATA INSPECTION  ‚ö†Ô∏è STOP HERE\n# Run this cell and share the output before continuing.\n# We need to verify data quality and set max_length correctly.\n# ============================================================\nprint(\"üìä DATA QUALITY INSPECTION\")\nprint(\"=\" * 60)\n\n# Check required output fields\nREQUIRED_FIELDS = ['primary_hypothesis', 'differential_diagnoses', 'key_symptoms', 'urgency', 'tests_ordered', 'reasoning']\nmissing_field_counts = {f: 0 for f in REQUIRED_FIELDS}\nurgency_counts = {}\nbad_examples = []\n\nfor i, ex in enumerate(examples):\n    out = ex.get('output', {})\n    for field in REQUIRED_FIELDS:\n        if field not in out:\n            missing_field_counts[field] += 1\n    urgency = out.get('urgency', 'MISSING')\n    urgency_counts[urgency] = urgency_counts.get(urgency, 0) + 1\n    if not ex.get('input') or not ex.get('output'):\n        bad_examples.append(i)\n\nprint(f\"\\nTotal examples: {len(examples)}\")\nprint(f\"Malformed examples (missing input or output): {len(bad_examples)}\")\nif bad_examples:\n    print(f\"  Indices: {bad_examples[:10]}\")\n\nprint(\"\\nMissing field counts (0 = all good):\")\nfor field, count in missing_field_counts.items():\n    status = \"‚úÖ\" if count == 0 else \"‚ùå\"\n    print(f\"  {status} {field}: {count} missing\")\n\nprint(\"\\nUrgency distribution:\")\nfor urgency, count in sorted(urgency_counts.items()):\n    pct = 100 * count / len(examples)\n    print(f\"  {urgency}: {count} ({pct:.1f}%)\")\n\nprint(\"\\nSample input (first example):\")\nprint(examples[0]['input'][:300] + \"...\")\nprint(\"\\nSample output (first example):\")\nprint(json.dumps(examples[0]['output'], indent=2)[:500])\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚ö†Ô∏è  SHARE THIS OUTPUT before running Cell 5+\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 5: Load Model & Tokenizer\n# ~1‚Äì2 min\n# ============================================================\nmodel_id = \"google/medgemma-1.5-4b-it\"\nprint(f\"\\nüî• Loading {model_id}...\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\nprint(\"‚úÖ Tokenizer loaded\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    dtype=torch.bfloat16,\n)\n\nprint(f\"‚úÖ Model loaded ({torch.cuda.memory_allocated()/1e9:.2f} GB VRAM used)\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 6: HuggingFace Login\n# Required for gated MedGemma model access\n# ============================================================\nfrom huggingface_hub import notebook_login\nnotebook_login()",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 7: Tokenize with Prompt Masking (v2 fix)\n#\n# KEY CHANGE FROM V1: We now set labels = -100 for the prompt\n# portion so the model only learns to predict the output, not\n# repeat back the input. This improves structured output quality.\n# ============================================================\n\nPROMPT_TEMPLATE = (\n    \"<start_of_turn>user\\n\"\n    \"Extract diagnostic information from this clinical note.\\n\\n\"\n    \"Clinical Note:\\n{note}<end_of_turn>\\n\"\n    \"<start_of_turn>model\\n\"\n)\n\ndef format_output(out):\n    \"\"\"Convert structured output dict to flat text format.\"\"\"\n    diff_dx = ', '.join(out.get('differential_diagnoses', []))\n    key_symptoms = ', '.join(out.get('key_symptoms', []))\n    tests = ', '.join(out.get('tests_ordered', []))\n    return (\n        f\"PRIMARY HYPOTHESIS: {out.get('primary_hypothesis', '')}\\n\"\n        f\"DIFFERENTIAL DIAGNOSES: {diff_dx}\\n\"\n        f\"KEY SUPPORTING EVIDENCE: {key_symptoms}\\n\"\n        f\"URGENCY LEVEL: {out.get('urgency', '')}\\n\"\n        f\"TESTS ORDERED: {tests}\\n\"\n        f\"CLINICAL REASONING: {out.get('reasoning', '')}\"\n    )\n\ndef tokenize_with_masking(ex, max_length=768):\n    \"\"\"Tokenize and mask prompt tokens in labels (output-only supervision).\"\"\"\n    note = ex['input']\n    output_text = format_output(ex['output'])\n\n    prompt = PROMPT_TEMPLATE.format(note=note)\n    full_text = prompt + output_text + \"<end_of_turn>\"\n\n    # Tokenize full sequence\n    tokenized = tokenizer(\n        full_text,\n        truncation=True,\n        max_length=max_length,\n        padding=False,\n        return_attention_mask=True,\n    )\n\n    # Tokenize prompt only to find its token length\n    prompt_tokenized = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=max_length,\n        padding=False,\n    )\n    prompt_len = len(prompt_tokenized['input_ids'])\n\n    # Labels: -100 for prompt tokens (ignored in loss), real ids for output\n    input_ids = tokenized['input_ids']\n    labels = [-100] * min(prompt_len, len(input_ids)) + input_ids[prompt_len:]\n\n    # Ensure labels same length as input_ids\n    labels = labels[:len(input_ids)]\n    if len(labels) < len(input_ids):\n        labels += [-100] * (len(input_ids) - len(labels))\n\n    tokenized['labels'] = labels\n    tokenized['token_type_ids'] = [0] * len(input_ids)\n\n    return tokenized\n\nprint(\"üîÑ Tokenizing examples with prompt masking...\")\nformatted = [tokenize_with_masking(ex) for ex in examples]\ndataset = Dataset.from_list(formatted)\n\nsplit = dataset.train_test_split(test_size=0.1, seed=42)\ntrain_data = split['train']\nval_data = split['test']\n\nprint(f\"‚úÖ Train: {len(train_data)}, Val: {len(val_data)}\")\nprint(f\"   Columns: {train_data.column_names}\")\nprint(f\"   Sample labels (first 10): {train_data[0]['labels'][:10]}  ‚Üê should be mostly -100\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 8: TOKEN LENGTH CHECK  ‚ö†Ô∏è STOP HERE\n# Run this and share the output.\n# If P95 > 700, we need to increase max_length and re-run Cell 7.\n# ============================================================\nprint(\"üìè TOKEN LENGTH ANALYSIS\")\nprint(\"=\" * 60)\n\nall_lengths = [len(ex['input_ids']) for ex in formatted]\nall_lengths.sort()\nn = len(all_lengths)\n\np50 = all_lengths[int(0.50 * n)]\np75 = all_lengths[int(0.75 * n)]\np90 = all_lengths[int(0.90 * n)]\np95 = all_lengths[int(0.95 * n)]\np99 = all_lengths[int(0.99 * n)]\nmax_len = all_lengths[-1]\nmin_len = all_lengths[0]\nmean_len = sum(all_lengths) // n\n\nprint(f\"Min:  {min_len}\")\nprint(f\"Mean: {mean_len}\")\nprint(f\"P50:  {p50}\")\nprint(f\"P75:  {p75}\")\nprint(f\"P90:  {p90}\")\nprint(f\"P95:  {p95}  ‚Üê KEY NUMBER\")\nprint(f\"P99:  {p99}\")\nprint(f\"Max:  {max_len}\")\n\ntruncated = sum(1 for ex in formatted if len(ex['input_ids']) >= 768)\nprint(f\"\\nExamples truncated at 768 tokens: {truncated} ({100*truncated/n:.1f}%)\")\n\nif p95 > 700:\n    print(\"\\n‚ö†Ô∏è  P95 > 700: Re-run Cell 7 with max_length=1024\")\nelif p95 > 500:\n    print(\"\\n‚úÖ P95 in range. Current max_length=768 is appropriate.\")\nelse:\n    print(\"\\n‚úÖ Sequences are short. Could reduce max_length=512 to speed up training.\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚ö†Ô∏è  SHARE THIS OUTPUT before running Cell 9+\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 9: LoRA Configuration\n# Identical to v1 (proven config)\n# ============================================================\nprint(\"üîß Applying LoRA...\")\n\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    inference_mode=False,\n)\n\nmodel = get_peft_model(model, lora_config)\ntrainable_params, total_params = model.get_nb_trainable_parameters()\nprint(f\"‚úÖ LoRA applied: {trainable_params:,} trainable / {total_params:,} total ({100 * trainable_params / total_params:.2f}%)\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 10: Training Arguments\n# Same as v1 ‚Äî batch_size=1, grad_accum=16, lr=2e-4, 3 epochs\n# warmup_steps replaces deprecated warmup_ratio\n# ============================================================\n\n# Estimated optimizer steps for warmup calculation\nestimated_steps_per_epoch = len(train_data) // 16  # grad_accum=16\ntotal_steps = estimated_steps_per_epoch * 3\nwarmup_steps = max(10, int(0.05 * total_steps))  # 5% warmup\n\nprint(f\"Estimated optimizer steps: {estimated_steps_per_epoch}/epoch, {total_steps} total\")\nprint(f\"Warmup steps: {warmup_steps}\")\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/medgemma-v2-checkpoints\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=warmup_steps,\n    fp16=False,\n    bf16=True,\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\",\n    dataloader_num_workers=0,\n    remove_unused_columns=False,\n    label_names=[\"labels\"],\n)\n\nprint(\"‚úÖ Training args ready\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 11: Data Collator + Trainer Setup\n# ============================================================\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List\n\n@dataclass\nclass PromptMaskedCollator:\n    \"\"\"Pads sequences while preserving -100 labels for prompt masking.\"\"\"\n    tokenizer: Any\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n        max_length = max(len(f['input_ids']) for f in features)\n\n        batch = {'input_ids': [], 'attention_mask': [], 'token_type_ids': [], 'labels': []}\n\n        for f in features:\n            pad_len = max_length - len(f['input_ids'])\n            batch['input_ids'].append(f['input_ids'] + [self.tokenizer.pad_token_id] * pad_len)\n            batch['attention_mask'].append(f['attention_mask'] + [0] * pad_len)\n            batch['token_type_ids'].append(f.get('token_type_ids', [0] * len(f['input_ids'])) + [0] * pad_len)\n            # Pad labels with -100 so padding is ignored in loss\n            batch['labels'].append(f['labels'] + [-100] * pad_len)\n\n        return {\n            'input_ids': torch.tensor(batch['input_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(batch['attention_mask'], dtype=torch.long),\n            'token_type_ids': torch.tensor(batch['token_type_ids'], dtype=torch.long),\n            'labels': torch.tensor(batch['labels'], dtype=torch.long),\n        }\n\ndata_collator = PromptMaskedCollator(tokenizer=tokenizer)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    processing_class=tokenizer,\n    data_collator=data_collator,\n)\n\nprint(\"‚úÖ Trainer ready\")\nprint(f\"   Train: {len(train_data)} examples\")\nprint(f\"   Val:   {len(val_data)} examples\")\nprint(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 12: üöÄ TRAIN\n# Expected: ~3‚Äì4 hours on T4\n# Watch for val_loss dropping each epoch.\n# v1 baseline: 1.348 ‚Üí 1.101 ‚Üí 1.075\n# v2 target:   ~1.1  ‚Üí ~0.9  ‚Üí ~0.8\n# ============================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"üöÄ STARTING FINE-TUNING v2\")\nprint(\"=\" * 70)\nprint(f\"Start: {datetime.now().strftime('%H:%M:%S')}\")\nprint(\"=\" * 70 + \"\\n\")\n\ntrain_start = datetime.now()\nresult = trainer.train()\ntrain_end = datetime.now()\n\nprint(f\"\\n‚úÖ Training complete!\")\nprint(f\"   Duration: {train_end - train_start}\")\nprint(f\"   Final train loss: {result.training_loss:.4f}\")\n\neval_result = trainer.evaluate()\nprint(f\"   Final val loss: {eval_result['eval_loss']:.4f}\")\nprint(f\"   v1 val loss was: 1.075 ‚Äî improvement: {1.075 - eval_result['eval_loss']:.3f}\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 13: Save Model\n# ============================================================\noutput_dir = \"/kaggle/working/medgemma-hypothesis-extraction-v2\"\nos.makedirs(output_dir, exist_ok=True)\n\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"‚úÖ Model saved to {output_dir}\")\nprint(\"\\nüìÅ Files:\")\nfor fname in os.listdir(output_dir):\n    size_mb = os.path.getsize(os.path.join(output_dir, fname)) / 1e6\n    print(f\"   {fname}: {size_mb:.1f} MB\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 14: Test Inference ‚Äî Compare v1 vs v2 Output Quality\n# ============================================================\nprint(\"\\nüß™ Testing fine-tuned model...\\n\")\n\n# Load base model + v2 adapter fresh\nbase = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    dtype=torch.bfloat16,\n)\nft_model = PeftModel.from_pretrained(base, output_dir)\nft_model.eval()\nprint(\"‚úÖ Fine-tuned model loaded\\n\")\n\n# Pick a held-out val example\ntest_ex = examples[0]  # Use first example; you can change to any index\nnote = test_ex['input']\nexpected = format_output(test_ex['output'])\n\nprompt = PROMPT_TEMPLATE.format(note=note)\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(ft_model.device)\n\nwith torch.no_grad():\n    outputs = ft_model.generate(\n        **inputs,\n        max_new_tokens=400,\n        temperature=0.1,\n        do_sample=False,\n        repetition_penalty=1.1,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract model turn only\nif \"<start_of_turn>model\" in response:\n    generated = response.split(\"<start_of_turn>model\")[-1].strip()\n    generated = generated.split(\"<end_of_turn>\")[0].strip()\nelse:\n    generated = response\n\nprint(\"=\" * 70)\nprint(\"EXPECTED OUTPUT:\")\nprint(\"=\" * 70)\nprint(expected)\nprint(\"\\n\" + \"=\" * 70)\nprint(\"FINE-TUNED MODEL (v2) OUTPUT:\")\nprint(\"=\" * 70)\nprint(generated)\n\n# Quick field check\nprint(\"\\n\" + \"=\" * 70)\nprint(\"üìä FIELD PRESENCE CHECK:\")\nfor field in [\"PRIMARY HYPOTHESIS\", \"DIFFERENTIAL DIAGNOSES\", \"KEY SUPPORTING EVIDENCE\", \"URGENCY LEVEL\", \"TESTS ORDERED\", \"CLINICAL REASONING\"]:\n    present = \"‚úÖ\" if field in generated else \"‚ùå\"\n    print(f\"  {present} {field}\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 15: Training Report\n# ============================================================\nreport = {\n    \"model_name\": \"medgemma-hypothesis-extraction-v2\",\n    \"base_model\": model_id,\n    \"training_date\": train_start.isoformat(),\n    \"training_duration\": str(train_end - train_start),\n    \"dataset\": {\n        \"file\": DATA_FILENAME,\n        \"total_examples\": len(examples),\n        \"train_examples\": len(train_data),\n        \"val_examples\": len(val_data),\n    },\n    \"training_config\": {\n        \"epochs\": 3,\n        \"learning_rate\": 2e-4,\n        \"lora_r\": 16,\n        \"lora_alpha\": 32,\n        \"batch_size\": 1,\n        \"gradient_accumulation\": 16,\n        \"warmup_steps\": warmup_steps,\n        \"max_seq_length\": 768,\n        \"prompt_masking\": True,\n    },\n    \"performance\": {\n        \"final_train_loss\": float(result.training_loss),\n        \"final_val_loss\": float(eval_result['eval_loss']),\n        \"v1_val_loss_for_comparison\": 1.075,\n        \"improvement_over_v1\": round(1.075 - float(eval_result['eval_loss']), 4),\n    },\n    \"usage_instructions\": {\n        \"prompt_template\": PROMPT_TEMPLATE,\n        \"generation_params\": {\n            \"max_new_tokens\": 400,\n            \"temperature\": 0.1,\n            \"do_sample\": False,\n            \"repetition_penalty\": 1.1,\n        },\n    },\n}\n\nreport_path = \"/kaggle/working/training_report_v2.json\"\nwith open(report_path, 'w') as f:\n    json.dump(report, f, indent=2)\n\nprint(\"‚úÖ Report saved to\", report_path)\nprint(json.dumps(report, indent=2))",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  }
 ]
}
