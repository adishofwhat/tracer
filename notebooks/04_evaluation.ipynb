{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# LoopGuard ‚Äî 8-Model Comparison Evaluation (v3)\n\n**Key changes from v2:**\n- Aggressive VRAM management between every model (explicit del + gc + empty_cache + synchronize)\n- Better response parsing per model family (fewer false negatives)\n- Real validation examples from training data (not synthetic test notes)\n- Meditron-7B added (EPFL, pretrained on clinical guidelines)\n- env var `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` set globally\n\n## Models (ordered by narrative arc)\n\n| # | Model | Size | Training data | Task FT |\n|---|-------|------|---------------|---------|\n| 1 | `google/gemma-2-2b-it` | 2B | General web | ‚ùå |\n| 2 | `meta-llama/Llama-3.2-3B-Instruct` | 3B | General web | ‚ùå |\n| 3 | `BioMistral/BioMistral-7B` | 7B | PubMed papers | ‚ùå |\n| 4 | `epfl-llm/meditron-7b` | 7B | PubMed + **clinical guidelines** | ‚ùå |\n| 5 | `Qwen/Qwen2.5-7B-Instruct` | 7B | General web | ‚ùå |\n| 6 | `deepseek-ai/DeepSeek-R1-Distill-Qwen-7B` | 7B | General + reasoning distill | ‚ùå |\n| 7 | `google/medgemma-1.5-4b-it` | 4B | Medical multimodal | ‚ùå |\n| 8 | **LoopGuard v2** | 4B | Medical + **421 clinical notes** | ‚úÖ |\n\n## Test set\n10 real examples held out from training data (last 2 from 5 different specialty batches + 1 nephrology).\nGround truth urgency labels come directly from the gold-standard output JSON.\nUrgency distribution: 5 high, 4 low, 1 medium.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 1: Install + set env vars\n# RESTART KERNEL after this cell\n# ============================================================\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n!pip uninstall -y -q transformers peft bitsandbytes accelerate 2>/dev/null\n!pip install -q transformers>=4.47.0\n!pip install -q peft>=0.13.0\n!pip install -q accelerate>=0.34.0\n!pip install -q bitsandbytes>=0.46.1\n!pip install -q matplotlib\n\nprint(\"‚úÖ Done. ‚ö†Ô∏è  RESTART KERNEL, then run from Cell 2.\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 2: Global setup ‚Äî run once after kernel restart\n# ============================================================\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport torch, json, re, gc, warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    props = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {props.name} | Total VRAM: {props.total_memory/1e9:.1f} GB\")\n\nBNB_CONFIG = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\ndef vram_free():\n    free = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n    return free / 1e9\n\ndef nuke_vram(*args):\n    \"\"\"Aggressively free all GPU memory.\"\"\"\n    for obj in args:\n        try:\n            del obj\n        except Exception:\n            pass\n    # Clear any lingering global model/tokenizer references\n    for name in ['model', 'base', 'tokenizer']:\n        if name in globals():\n            try:\n                del globals()[name]\n            except Exception:\n                pass\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(f\"   VRAM after clear: {vram_free():.2f} GB free\")\n\nALL_RESULTS = {}\nprint(\"\\n‚úÖ Global setup complete\")\nprint(f\"   VRAM available: {vram_free():.2f} GB\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 3: Test notes + scoring functions\n# 10 REAL examples from training batches (last N of each file)\n# Ground truth urgency from gold-standard outputs\n# ============================================================\n\nTEST_NOTES = [\n    {\n        \"id\": \"SEP01\", \"urgency_gt\": \"high\",\n        \"primary_hypothesis_gt\": \"Ascending cholangitis (post-operative complication)\",\n        \"note\": \"62-year-old male with recent hospitalization for cholecystectomy (7 days ago) presents to ED with fever (102.1¬∞F), right upper quadrant pain, and jaundice. Reports incision looks okay but has had worsening abdominal pain. Vitals: Temp 102.1¬∞F, HR 106, BP 118/72, RR 18. Exam: Jaundiced, RUQ tenderness, surgical site clean without drainage. Murphy's sign negative (post-op). Labs show elevated bilirubin and transaminases. Concern for biliary leak or retained stone with cholangitis.\"\n    },\n    {\n        \"id\": \"SEP02\", \"urgency_gt\": \"high\",\n        \"primary_hypothesis_gt\": \"Infective endocarditis (likely tricuspid valve)\",\n        \"note\": \"41-year-old female injection drug user presents with 3 days of fever (103.6¬∞F), rigors, and new heart murmur. Reports feeling weak and having night sweats. Denies recent hospitalizations but admits to IV heroin use. Vitals: Temp 103.6¬∞F, HR 118, BP 102/58, RR 18, SpO2 96%. Exam: III/VI systolic murmur at apex (new per patient), scattered petechiae on palms, splinter hemorrhages in nails. Assessment: Likely infective endocarditis. Plan: 3 sets blood cultures from different sites, echocardiogram, admit for IV antibiotics.\"\n    },\n    {\n        \"id\": \"NEU01\", \"urgency_gt\": \"high\",\n        \"primary_hypothesis_gt\": \"Fat embolism syndrome\",\n        \"note\": \"72-year-old male post-op day 3 from hip replacement surgery presents with sudden dyspnea, confusion, and petechial rash on chest. Nurse reports he was fine 2 hours ago. Vitals: BP 94/58, HR 128, RR 32, SpO2 86% on 4L NC, Temp 100.8¬∞F. Exam: Confused, tachypneic, petechiae on anterior chest and conjunctiva, decreased breath sounds bilaterally. ABG: pH 7.32, PaCO2 32, PaO2 58. Assessment: Fat embolism syndrome. Plan: Supportive care, mechanical ventilation likely needed, orthopedic and ICU consult, consider steroids.\"\n    },\n    {\n        \"id\": \"CAR01\", \"urgency_gt\": \"high\",\n        \"primary_hypothesis_gt\": \"Critical aortic stenosis with exertional syncope\",\n        \"note\": \"56-year-old male with known severe aortic stenosis presents with syncope while climbing stairs. Reports 3-month history of progressive exertional dyspnea and chest pressure. Vitals: BP 98/64, HR 62, RR 18, SpO2 96%. Exam: Delayed carotid upstroke, sustained PMI, harsh systolic crescendo-decrescendo murmur at right upper sternal border radiating to carotids, paradoxical S2 splitting. Echocardiogram 6 months ago: severe AS, valve area 0.7 cm¬≤, mean gradient 52 mmHg, EF 55%. Assessment: Critical aortic stenosis with syncope. Plan: Admit telemetry, cardiology and cardiac surgery consult for urgent valve replacement, avoid dehydration and vasodilators.\"\n    },\n    {\n        \"id\": \"CAR02\", \"urgency_gt\": \"high\",\n        \"primary_hypothesis_gt\": \"Acute-on-chronic subdural hematoma with anticoagulation\",\n        \"note\": \"81-year-old male on warfarin for atrial fibrillation fell down stairs 6 hours ago. Initially seemed fine but now increasingly confused and lethargic. Family reports worsening headache. PMH: Atrial fibrillation on warfarin (INR usually 2-3). Vitals: BP 158/88, HR 72. Exam: Lethargic, GCS 13, confused, left-sided weakness 3/5, pupils equal and reactive. CT head: Right-sided acute-on-chronic subdural hematoma, 1.5cm thickness, 8mm midline shift. INR: 3.8. Assessment: Acute subdural hematoma on chronic subdural with coagulopathy. Plan: Reverse anticoagulation (vitamin K, PCC or FFP), neurosurgery consult for evacuation, admit ICU.\"\n    },\n    {\n        \"id\": \"LOW01\", \"urgency_gt\": \"low\",\n        \"primary_hypothesis_gt\": \"Vitamin D deficiency, corrected with supplementation\",\n        \"note\": \"50-year-old male presents for vitamin D deficiency follow-up. Was started on vitamin D3 2000 IU daily 3 months ago. Repeat labs show 25-OH vitamin D level increased from 18 to 35 ng/mL (goal 30-50). Reports no bone pain, no muscle weakness. PMH: Vitamin D deficiency. Assessment: Vitamin D insufficiency, now replete. Plan: Continue vitamin D3 1000-2000 IU daily for maintenance, recheck level in 6-12 months, ensure adequate calcium intake, weight-bearing exercise for bone health.\"\n    },\n    {\n        \"id\": \"LOW02\", \"urgency_gt\": \"low\",\n        \"primary_hypothesis_gt\": \"Polycystic ovary syndrome, stable on medical management\",\n        \"note\": \"28-year-old female with polycystic ovary syndrome (PCOS) on metformin 1000mg twice daily and combined oral contraceptive presents for follow-up. Reports regular menstrual cycles on OCP, no hirsutism worsening, lost 8 lbs with diet and exercise. Glucose tolerance test normal. Assessment: PCOS, managed with lifestyle and medications. Plan: Continue metformin and OCP, continue weight loss efforts (goal BMI <25), screen for metabolic syndrome annually, follow-up in 6 months.\"\n    },\n    {\n        \"id\": \"SCR01\", \"urgency_gt\": \"low\",\n        \"primary_hypothesis_gt\": \"Routine breast cancer screening with dense breast tissue\",\n        \"note\": \"50-year-old female presents for annual mammogram. Last mammogram 1 year ago showed dense breast tissue, otherwise normal. No breast symptoms, no masses palpated. PMH: None. Family history: Maternal aunt with breast cancer at age 65. Vitals: Normal. Exam: Normal breast exam, no masses or lymphadenopathy. Assessment: Routine breast cancer screening, dense breasts. Plan: Screening mammogram, consider supplemental screening (ultrasound or MRI) if extremely dense (category D), breast self-awareness education, continue annual screening.\"\n    },\n    {\n        \"id\": \"SCR02\", \"urgency_gt\": \"low\",\n        \"primary_hypothesis_gt\": \"Generalized anxiety disorder, mild\",\n        \"note\": \"28-year-old male presents for anxiety screening. Reports feeling anxious at work, difficulty concentrating, occasional palpitations. No panic attacks. Sleep and appetite normal. PMH: None. Medications: None. Vitals: BP 128/82, HR 88. Exam: Normal. GAD-7 score: 9 (mild anxiety). Assessment: Mild anxiety symptoms, screening positive. Plan: Lifestyle counseling (exercise, sleep hygiene, caffeine reduction, stress management), cognitive behavioral therapy referral, relaxation techniques, consider SSRI if symptoms worsen or persist, follow-up in 1 month.\"\n    },\n    {\n        \"id\": \"NEP01\", \"urgency_gt\": \"medium\",\n        \"primary_hypothesis_gt\": \"HIV-associated nephropathy (HIVAN)\",\n        \"note\": \"38-year-old female with HIV (CD4 200, viral load 85,000, non-adherent to ART) presents with 4-week history of progressive lower extremity edema and foamy urine. Reports fatigue. Denies hematuria or dysuria. PMH: HIV diagnosed 3 years ago. Medications: None currently. Vitals: BP 148/92. Exam: Facial edema, significant lower extremity edema. Labs: Creatinine 2.6, albumin 2.0, CD4 200, urinalysis 4+ protein, urine protein 8.5 g/day. Kidney biopsy: Collapsing variant of FSGS. Assessment: HIV-associated nephropathy (HIVAN). Plan: Immediate initiation of antiretroviral therapy, ACE inhibitor, prednisone, nephrology and infectious disease consult.\"\n    },\n]\n\nprint(f\"‚úÖ {len(TEST_NOTES)} real validation notes loaded\")\nprint(f\"   high={sum(1 for n in TEST_NOTES if n['urgency_gt']=='high')}, \"\n      f\"medium={sum(1 for n in TEST_NOTES if n['urgency_gt']=='medium')}, \"\n      f\"low={sum(1 for n in TEST_NOTES if n['urgency_gt']=='low')}\")\n\n# ---- Field definitions ----\n# We score for 6 fields. The field names are flexible ‚Äî any reasonable variant counts.\nFIELD_PATTERNS = [\n    r'primary[\\s_-]*hypothesis',\n    r'differential[\\s_-]*diagnos',\n    r'key[\\s_-]*(supporting|symptoms|evidence|findings)',\n    r'urgency[\\s_-]*(level|classification|rating)?',\n    r'tests?[\\s_-]*(ordered|recommended|planned|to[\\s_-]*order)',\n    r'(clinical[\\s_-]*)?reasoning',\n]\nREQUIRED_FIELD_COUNT = len(FIELD_PATTERNS)\n\ndef count_fields(text):\n    \"\"\"Count how many of the 6 required fields appear in the output.\"\"\"\n    text_lower = text.lower()\n    return sum(1 for pattern in FIELD_PATTERNS if re.search(pattern, text_lower))\n\ndef extract_urgency(text):\n    \"\"\"Extract urgency level from model output.\"\"\"\n    text_lower = text.lower()\n    # Pattern: 'urgency level: high' or 'urgency: medium' etc.\n    m = re.search(r'urgency[\\s_-]*(level|classification)?[:\\s]+([\\w]+)', text_lower)\n    if m:\n        val = m.group(2).strip()\n        if val in ('high', 'medium', 'low'):\n            return val\n    # Fallback: look for urgency word close to level word\n    for u in ('high', 'medium', 'low'):\n        if re.search(rf'urgency.*\\b{u}\\b', text_lower):\n            return u\n    return 'unknown'\n\ndef score_output(text, ground_truth_urgency):\n    fields = count_fields(text)\n    urgency = extract_urgency(text)\n    return {\n        'fields_present': fields,\n        'completeness_pct': round(100 * fields / REQUIRED_FIELD_COUNT),\n        'valid_structure': fields >= 5,\n        'urgency_detected': urgency,\n        'urgency_match': urgency == ground_truth_urgency.lower(),\n    }\n\ndef run_eval(model, tokenizer, prompt_fn, display_name, decode_fn=None):\n    \"\"\"Run evaluation on all 10 test notes. prompt_fn(note_text) -> prompt string.\"\"\"\n    results = []\n    for i, note in enumerate(TEST_NOTES):\n        prompt = prompt_fn(note['note'])\n        inputs = tokenizer(prompt, return_tensors='pt', truncation=True,\n                           max_length=768).to(model.device)\n        with torch.no_grad():\n            out = model.generate(\n                **inputs, max_new_tokens=450, do_sample=False,\n                repetition_penalty=1.1,\n                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n            )\n        raw = tokenizer.decode(out[0], skip_special_tokens=True)\n        if decode_fn:\n            generated = decode_fn(raw, prompt)\n        else:\n            # Default: strip prompt prefix\n            generated = raw[len(prompt):].strip() if raw.startswith(prompt) else raw.strip()\n        scores = score_output(generated, note['urgency_gt'])\n        results.append({'note_id': note['id'], 'urgency_gt': note['urgency_gt'], **scores})\n        tick = '‚úÖ' if scores['urgency_match'] else '‚ùå'\n        print(f\"  [{i+1:02d}/10] {note['id']} \"\n              f\"fields={scores['fields_present']}/6 \"\n              f\"urgency={scores['urgency_detected']} {tick}\")\n    avg_comp = sum(r['completeness_pct'] for r in results) / len(results)\n    pct_valid = 100 * sum(1 for r in results if r['valid_structure']) / len(results)\n    pct_urg = 100 * sum(1 for r in results if r['urgency_match']) / len(results)\n    summary = {\n        'model': display_name,\n        'avg_completeness': round(avg_comp, 1),\n        'pct_valid_structure': round(pct_valid, 1),\n        'pct_urgency_correct': round(pct_urg, 1),\n        'per_note': results,\n    }\n    ALL_RESULTS[display_name] = summary\n    print(f\"\\n  üìä {display_name.replace(chr(10),' ')}\")\n    print(f\"     Completeness: {avg_comp:.1f}% | Valid structure: {pct_valid:.1f}% | Urgency acc: {pct_urg:.1f}%\")\n    return summary\n\n# Standard instruction used for all models (except Meditron which is base-only)\nSTD_INSTRUCTION = (\n    \"Extract diagnostic information from this clinical note.\\n\\n\"\n    \"Clinical Note:\\n{note}\\n\\n\"\n    \"Output ONLY these 6 fields:\\n\"\n    \"PRIMARY HYPOTHESIS: [main diagnosis]\\n\"\n    \"DIFFERENTIAL DIAGNOSES: [comma-separated alternatives]\\n\"\n    \"KEY SUPPORTING EVIDENCE: [comma-separated findings]\\n\"\n    \"URGENCY LEVEL: [high/medium/low]\\n\"\n    \"TESTS ORDERED: [comma-separated tests]\\n\"\n    \"CLINICAL REASONING: [brief explanation]\"\n)\n\nprint(\"\\n‚úÖ Scoring functions and test notes ready\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 4: MODEL 1 ‚Äî Gemma 2 2B-it\n# Chat template: <start_of_turn>user/model\n# ~10 min\n# ============================================================\nDN = \"Gemma 2 2B\\n(general, no medical)\"\nprint(f\"\\nüî¨ [1/8] google/gemma-2-2b-it\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2-2b-it\", quantization_config=BNB_CONFIG,\n    device_map=\"auto\", dtype=torch.bfloat16)\nmodel.eval()\nprint(f\"‚úÖ Loaded | VRAM used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\ndef prompt_gemma(note):\n    msgs = [{\"role\": \"user\", \"content\": STD_INSTRUCTION.format(note=note)}]\n    return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n\ndef decode_gemma(raw, prompt):\n    if \"<start_of_turn>model\" in raw:\n        text = raw.split(\"<start_of_turn>model\")[-1]\n        return text.split(\"<end_of_turn>\")[0].strip()\n    return raw[len(prompt):].strip()\n\nrun_eval(model, tokenizer, prompt_gemma, DN, decode_gemma)\nnuke_vram(model, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 5: MODEL 2 ‚Äî Llama 3.2 3B Instruct\n# Requires HF token. If not approved yet, skip and come back.\n# Chat template: <|begin_of_text|><|start_header_id|> format\n# ~10 min\n# ============================================================\nDN = \"Llama 3.2 3B\\n(general, no medical)\"\nprint(f\"\\nüî¨ [2/8] meta-llama/Llama-3.2-3B-Instruct\")\nprint(f\"   VRAM available: {vram_free():.2f} GB\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.2-3B-Instruct\", quantization_config=BNB_CONFIG,\n    device_map=\"auto\", dtype=torch.bfloat16)\nmodel.eval()\nprint(f\"‚úÖ Loaded | VRAM used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\ndef prompt_llama(note):\n    msgs = [\n        {\"role\": \"system\", \"content\": \"You are a clinical documentation assistant. Output ONLY the structured format requested.\"},\n        {\"role\": \"user\", \"content\": STD_INSTRUCTION.format(note=note)}\n    ]\n    return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n\ndef decode_llama(raw, prompt):\n    # Strip eot tokens, extract assistant content\n    raw = raw.replace(\"<|eot_id|>\", \"\").strip()\n    # Split on assistant header\n    parts = re.split(r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>', raw, flags=re.IGNORECASE)\n    if len(parts) > 1:\n        return parts[-1].strip()\n    # Fallback: strip prompt\n    return raw[len(prompt):].strip()\n\nrun_eval(model, tokenizer, prompt_llama, DN, decode_llama)\nnuke_vram(model, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 6: MODEL 3 ‚Äî BioMistral 7B\n# Mistral [INST]...[/INST] format, NO system message\n# Note: loads pytorch_model.bin (not safetensors), auto-conversion\n# error in background thread is harmless ‚Äî model loads fine\n# ~12 min\n# ============================================================\nDN = \"BioMistral 7B\\n(medical, PubMed pretrain)\"\nprint(f\"\\nüî¨ [3/8] BioMistral/BioMistral-7B\")\nprint(f\"   VRAM available: {vram_free():.2f} GB\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"BioMistral/BioMistral-7B\")\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"BioMistral/BioMistral-7B\", quantization_config=BNB_CONFIG,\n    device_map=\"auto\", dtype=torch.bfloat16)\nmodel.eval()\nprint(f\"‚úÖ Loaded | VRAM used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\ndef prompt_biomistral(note):\n    # No system message for BioMistral\n    content = STD_INSTRUCTION.format(note=note)\n    msgs = [{\"role\": \"user\", \"content\": content}]\n    try:\n        return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n    except Exception:\n        return f\"<s>[INST] {content} [/INST]\"\n\ndef decode_biomistral(raw, prompt):\n    if \"[/INST]\" in raw:\n        return raw.split(\"[/INST]\")[-1].strip()\n    return raw[len(prompt):].strip()\n\nrun_eval(model, tokenizer, prompt_biomistral, DN, decode_biomistral)\nnuke_vram(model, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 7: MODEL 4 ‚Äî Meditron 7B (EPFL)\n# BASE model ‚Äî no instruction tuning. Raw completion format.\n# Pretrained on PubMed + 46K clinical guidelines (closest to\n# clinical data without task fine-tuning)\n# ~12 min\n# ============================================================\nDN = \"Meditron 7B\\n(medical, guidelines pretrain)\"\nprint(f\"\\nüî¨ [4/8] epfl-llm/meditron-7b\")\nprint(f\"   VRAM available: {vram_free():.2f} GB\")\nprint(\"   Base model only ‚Äî using raw completion format (no chat template)\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"epfl-llm/meditron-7b\")\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"epfl-llm/meditron-7b\", quantization_config=BNB_CONFIG,\n    device_map=\"auto\", dtype=torch.bfloat16)\nmodel.eval()\nprint(f\"‚úÖ Loaded | VRAM used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\n# Meditron is a base model ‚Äî prompt as raw completion ending on first field label\n# so the model continues the structured format\nMEDITRON_PROMPT = (\n    \"### Clinical Note:\\n{note}\\n\\n\"\n    \"### Diagnostic Extraction:\\n\"\n    \"PRIMARY HYPOTHESIS:\"\n)\n\ndef prompt_meditron(note):\n    return MEDITRON_PROMPT.format(note=note)\n\ndef decode_meditron(raw, prompt):\n    # Strip prompt, then prepend the field label we used as prompt suffix\n    generated = raw[len(prompt):].strip() if raw.startswith(prompt) else raw.strip()\n    return \"PRIMARY HYPOTHESIS:\" + generated\n\nrun_eval(model, tokenizer, prompt_meditron, DN, decode_meditron)\nnuke_vram(model, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 8: MODEL 5 ‚Äî Qwen2.5 7B Instruct\n# ChatML <|im_start|> format via apply_chat_template\n# ~12 min\n# ============================================================\nDN = \"Qwen2.5 7B\\n(general, best <10B)\"\nprint(f\"\\nüî¨ [5/8] Qwen/Qwen2.5-7B-Instruct\")\nprint(f\"   VRAM available: {vram_free():.2f} GB\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2.5-7B-Instruct\", quantization_config=BNB_CONFIG,\n    device_map=\"auto\", dtype=torch.bfloat16)\nmodel.eval()\nprint(f\"‚úÖ Loaded | VRAM used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\ndef prompt_qwen(note):\n    msgs = [\n        {\"role\": \"system\", \"content\": \"You are a clinical documentation assistant. Output ONLY the structured format requested. No preamble.\"},\n        {\"role\": \"user\", \"content\": STD_INSTRUCTION.format(note=note)}\n    ]\n    return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n\ndef decode_qwen(raw, prompt):\n    # Qwen outputs only new tokens cleanly with input_ids slicing\n    # but since we decode full sequence, strip prompt-equivalent\n    if \"<|im_start|>assistant\" in raw:\n        text = raw.split(\"<|im_start|>assistant\")[-1]\n        return text.replace(\"<|im_end|>\", \"\").strip()\n    return raw[len(prompt):].strip()\n\nrun_eval(model, tokenizer, prompt_qwen, DN, decode_qwen)\nnuke_vram(model, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 9: MODEL 6 ‚Äî DeepSeek-R1 Distill Qwen 7B\n# ChatML (Qwen2.5 tokenizer) via apply_chat_template\n# Per DeepSeek docs: NO system prompt, all in user turn\n# Produces <think>...</think> reasoning blocks ‚Äî strip before scoring\n# ~15 min\n# ============================================================\nDN = \"DeepSeek-R1 7B\\n(reasoning, no medical)\"\nprint(f\"\\nüî¨ [6/8] deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\nprint(f\"   VRAM available: {vram_free():.2f} GB\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", quantization_config=BNB_CONFIG,\n    device_map=\"auto\", dtype=torch.bfloat16)\nmodel.eval()\nprint(f\"‚úÖ Loaded | VRAM used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\ndef prompt_deepseek(note):\n    # No system message per DeepSeek-R1 official docs\n    msgs = [{\"role\": \"user\", \"content\": STD_INSTRUCTION.format(note=note)}]\n    return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n\ndef decode_deepseek(raw, prompt):\n    if \"<|im_start|>assistant\" in raw:\n        text = raw.split(\"<|im_start|>assistant\")[-1]\n        text = text.replace(\"<|im_end|>\", \"\").strip()\n    else:\n        text = raw[len(prompt):].strip()\n    # Strip <think>...</think> reasoning blocks\n    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n    return text\n\nrun_eval(model, tokenizer, prompt_deepseek, DN, decode_deepseek)\nnuke_vram(model, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 10: MODEL 7 ‚Äî Base MedGemma 1.5 4B-it (zero-shot)\n# Same base model as LoopGuard ‚Äî proves fine-tuning adds value\n# ~10 min\n# ============================================================\nDN = \"Base MedGemma 4B\\n(medical, zero-shot)\"\nprint(f\"\\nüî¨ [7/8] google/medgemma-1.5-4b-it (zero-shot)\")\nprint(f\"   VRAM available: {vram_free():.2f} GB\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/medgemma-1.5-4b-it\")\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/medgemma-1.5-4b-it\", quantization_config=BNB_CONFIG,\n    device_map=\"auto\", dtype=torch.bfloat16)\nmodel.eval()\nprint(f\"‚úÖ Loaded | VRAM used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\ndef prompt_medgemma(note):\n    msgs = [{\"role\": \"user\", \"content\": STD_INSTRUCTION.format(note=note)}]\n    return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n\ndef decode_gemma(raw, prompt):\n    if \"<start_of_turn>model\" in raw:\n        text = raw.split(\"<start_of_turn>model\")[-1]\n        return text.split(\"<end_of_turn>\")[0].strip()\n    return raw[len(prompt):].strip()\n\nrun_eval(model, tokenizer, prompt_medgemma, DN, decode_gemma)\nnuke_vram(model, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 11: MODEL 8 ‚Äî LoopGuard v2 (fine-tuned MedGemma)\n# Load base + LoRA adapter from Kaggle model or /kaggle/working\n# ADAPTER_DIR: update path if you uploaded adapter as Kaggle model\n# ~10 min\n# ============================================================\nBASE_ID = \"google/medgemma-1.5-4b-it\"\n# Update this path if you uploaded adapter as a Kaggle model:\nADAPTER_DIR = \"/kaggle/working/medgemma-hypothesis-extraction-v2\"\n# Fallback: check Kaggle model mount\nif not os.path.exists(ADAPTER_DIR):\n    import glob\n    candidates = glob.glob(\"/kaggle/input/**/adapter_config.json\", recursive=True)\n    if candidates:\n        ADAPTER_DIR = os.path.dirname(candidates[0])\n        print(f\"   Found adapter at: {ADAPTER_DIR}\")\n    else:\n        raise FileNotFoundError(f\"Adapter not found. Upload medgemma-loopguard-v2 as Kaggle model and add to notebook.\")\n\nDN = \"LoopGuard v2\\n(fine-tuned MedGemma)\"\nprint(f\"\\nüî¨ [8/8] LoopGuard v2\")\nprint(f\"   Base: {BASE_ID}\")\nprint(f\"   Adapter: {ADAPTER_DIR}\")\nprint(f\"   VRAM available: {vram_free():.2f} GB\")\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_ID)\ntokenizer.pad_token = tokenizer.eos_token\nbase = AutoModelForCausalLM.from_pretrained(\n    BASE_ID, quantization_config=BNB_CONFIG,\n    device_map=\"auto\", dtype=torch.bfloat16)\nmodel = PeftModel.from_pretrained(base, ADAPTER_DIR)\nmodel.eval()\nprint(f\"‚úÖ Loaded with adapter | VRAM used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\n# Use the exact prompt template from fine-tuning\nLOOPGUARD_TMPL = (\n    \"<start_of_turn>user\\n\"\n    \"Extract diagnostic information from this clinical note.\\n\\n\"\n    \"Clinical Note:\\n{note}<end_of_turn>\\n\"\n    \"<start_of_turn>model\\n\"\n)\n\ndef prompt_loopguard(note):\n    return LOOPGUARD_TMPL.format(note=note)\n\ndef decode_loopguard(raw, prompt):\n    if \"<start_of_turn>model\" in raw:\n        text = raw.split(\"<start_of_turn>model\")[-1]\n        return text.split(\"<end_of_turn>\")[0].strip()\n    return raw[len(prompt):].strip()\n\nrun_eval(model, tokenizer, prompt_loopguard, DN, decode_loopguard)\nnuke_vram(model, base, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 12: Score Table\n# ============================================================\nORDERED_MODELS = [\n    \"Gemma 2 2B\\n(general, no medical)\",\n    \"Llama 3.2 3B\\n(general, no medical)\",\n    \"BioMistral 7B\\n(medical, PubMed pretrain)\",\n    \"Meditron 7B\\n(medical, guidelines pretrain)\",\n    \"Qwen2.5 7B\\n(general, best <10B)\",\n    \"DeepSeek-R1 7B\\n(reasoning, no medical)\",\n    \"Base MedGemma 4B\\n(medical, zero-shot)\",\n    \"LoopGuard v2\\n(fine-tuned MedGemma)\",\n]\n\nprint(\"\\n\" + \"=\" * 82)\nprint(f\"{'MODEL':<40} {'COMPLETENESS':>13} {'VALID STRUCT':>13} {'URGENCY ACC':>13}\")\nprint(\"=\" * 82)\nfor m in ORDERED_MODELS:\n    if m not in ALL_RESULTS:\n        print(f\"{m.replace(chr(10),' '):<40} {'NOT RUN':>41}\")\n        continue\n    s = ALL_RESULTS[m]\n    name = m.replace('\\n', ' ')\n    mark = \" ‚úÖ\" if \"LoopGuard\" in m else \"\"\n    print(f\"{name:<40} {s['avg_completeness']:>12.1f}% {s['pct_valid_structure']:>12.1f}% {s['pct_urgency_correct']:>12.1f}%{mark}\")\nprint(\"=\" * 82)\n\nif \"LoopGuard v2\\n(fine-tuned MedGemma)\" in ALL_RESULTS:\n    ft = ALL_RESULTS[\"LoopGuard v2\\n(fine-tuned MedGemma)\"]\n    others = [v for k, v in ALL_RESULTS.items() if \"LoopGuard\" not in k]\n    if others:\n        best_comp = max(v['avg_completeness'] for v in others)\n        best_urg  = max(v['pct_urgency_correct'] for v in others)\n        print(f\"\\nLoopGuard vs best competitor:\")\n        print(f\"  Completeness: {ft['avg_completeness']:.1f}% vs {best_comp:.1f}% (+{ft['avg_completeness']-best_comp:.1f}pp)\")\n        print(f\"  Urgency acc:  {ft['pct_urgency_correct']:.1f}% vs {best_urg:.1f}% ({ft['pct_urgency_correct']-best_urg:+.1f}pp)\")\n\n# Save\nwith open('/kaggle/working/eval_comparison_8models.json', 'w') as f:\n    json.dump({k: {\"summary\": {\"model\": v[\"model\"],\n                               \"avg_completeness\": v[\"avg_completeness\"],\n                               \"pct_valid_structure\": v[\"pct_valid_structure\"],\n                               \"pct_urgency_correct\": v[\"pct_urgency_correct\"]},\n                   \"per_note\": v[\"per_note\"]} for k, v in ALL_RESULTS.items()}, f, indent=2)\nprint(\"\\n‚úÖ Saved ‚Üí /kaggle/working/eval_comparison_8models.json\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 13: Bar Chart ‚Üí eval_comparison.png\n# ============================================================\nmodels_present = [m for m in ORDERED_MODELS if m in ALL_RESULTS]\nif not models_present:\n    print(\"No results to plot yet.\")\nelse:\n    summaries = [ALL_RESULTS[m] for m in models_present]\n    labels = [m.replace('\\n', '\\n') for m in models_present]\n\n    def get_color(m):\n        if \"LoopGuard\" in m: return '#28a745'\n        if any(x in m for x in [\"MedGemma\", \"BioMistral\", \"Meditron\"]): return '#4a90d9'\n        return '#9e9e9e'\n    colors = [get_color(m) for m in models_present]\n\n    metrics = [\n        (\"avg_completeness\",    \"Field Completeness (%)\",  \"All 6 fields present\"),\n        (\"pct_valid_structure\", \"Valid Structure (%)\",      \"‚â•5/6 fields present\"),\n        (\"pct_urgency_correct\", \"Urgency Accuracy (%)\",     \"Matches ground truth\"),\n    ]\n\n    fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n    fig.patch.set_facecolor('#f8f9fa')\n\n    for ax, (key, title, desc) in zip(axes, metrics):\n        values = [s[key] for s in summaries]\n        x = np.arange(len(labels))\n        bars = ax.bar(x, values, color=colors, width=0.65, zorder=3,\n                      edgecolor='white', linewidth=0.5)\n        ax.set_xticks(x)\n        ax.set_xticklabels(labels, fontsize=7)\n        ax.set_ylim(0, 118)\n        ax.set_ylabel(\"Score (%)\", fontsize=10)\n        ax.set_title(title, fontsize=12, fontweight='bold', pad=10)\n        ax.set_facecolor('#ffffff')\n        ax.grid(axis='y', alpha=0.35, zorder=0)\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        for bar, val in zip(bars, values):\n            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1.5,\n                    f'{val:.0f}%', ha='center', va='bottom', fontsize=8.5, fontweight='bold')\n        ax.text(0.5, -0.20, desc, transform=ax.transAxes,\n                ha='center', fontsize=8, color='#666', style='italic')\n\n    legend_items = [\n        mpatches.Patch(color='#9e9e9e', label='General model (no medical pretraining)'),\n        mpatches.Patch(color='#4a90d9', label='Medical model (no task fine-tuning)'),\n        mpatches.Patch(color='#28a745', label='LoopGuard v2 ‚Äî MedGemma + clinical note fine-tuning'),\n    ]\n    fig.legend(handles=legend_items, loc='upper center',\n               bbox_to_anchor=(0.5, 1.04), ncol=3, fontsize=9, frameon=True)\n    fig.suptitle(\n        'LoopGuard v2 vs 7 Comparable Open-Source Models (all ‚â§7B, locally deployable)\\n'\n        '10 Real Clinical Notes ¬∑ 3 Metrics ¬∑ Zero-shot except LoopGuard v2',\n        fontsize=11, fontweight='bold', y=1.10)\n\n    plt.tight_layout()\n    plt.savefig('/kaggle/working/eval_comparison.png', dpi=150,\n                bbox_inches='tight', facecolor=fig.get_facecolor())\n    print(\"‚úÖ Chart saved ‚Üí /kaggle/working/eval_comparison.png\")\n    plt.show()",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 14: Writeup-ready text\n# ============================================================\nprint(\"\\nüìù COPY THIS INTO WRITEUP ‚Äî Technical Details section\\n\")\n\nprint(\"| Model | Size | Completeness | Valid Structure | Urgency Accuracy |\")\nprint(\"|-------|------|-------------|-----------------|------------------|\")\nfor m in ORDERED_MODELS:\n    if m not in ALL_RESULTS:\n        continue\n    s = ALL_RESULTS[m]\n    name = m.replace('\\n', ' ')\n    # Extract size from display name\n    size = \"4B\" if \"MedGemma\" in m or \"LoopGuard\" in m else (\"2B\" if \"2B\" in m else (\"3B\" if \"3B\" in m else \"7B\"))\n    mark = \" ‚úÖ\" if \"LoopGuard\" in m else \"\"\n    print(f\"| {name}{mark} | {size} | {s['avg_completeness']:.0f}% | {s['pct_valid_structure']:.0f}% | {s['pct_urgency_correct']:.0f}% |\")\n\nprint(\"\\nüìä KEY CLAIMS FOR WRITEUP:\")\nif \"LoopGuard v2\\n(fine-tuned MedGemma)\" in ALL_RESULTS:\n    ft = ALL_RESULTS[\"LoopGuard v2\\n(fine-tuned MedGemma)\"]\n    others = {k: v for k, v in ALL_RESULTS.items() if \"LoopGuard\" not in k}\n    if others:\n        best_k = max(others, key=lambda k: others[k]['avg_completeness'])\n        print(f\"  - LoopGuard completeness: {ft['avg_completeness']:.0f}% vs best competitor ({best_k.replace(chr(10),' ')}): {others[best_k]['avg_completeness']:.0f}%\")\n        print(f\"  - LoopGuard urgency accuracy: {ft['pct_urgency_correct']:.0f}%\")\n        print(f\"  - LoopGuard valid structure rate: {ft['pct_valid_structure']:.0f}%\")\n        med_models = {k: v for k, v in others.items() if any(x in k for x in ['MedGemma', 'BioMistral', 'Meditron'])}\n        if med_models:\n            best_med = max(med_models, key=lambda k: med_models[k]['avg_completeness'])\n            print(f\"  - vs best medical non-FT ({best_med.replace(chr(10),' ')}): completeness {med_models[best_med]['avg_completeness']:.0f}% ‚Üí FT adds +{ft['avg_completeness']-med_models[best_med]['avg_completeness']:.0f}pp\")\n\nprint(\"\\n‚úÖ Download: eval_comparison.png + eval_comparison_8models.json\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  }
 ]
}
