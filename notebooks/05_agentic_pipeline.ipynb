{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Setup\n",
    "# ============================================================\n",
    "import json, re, torch, gc, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# â”€â”€ UPDATE THESE PATHS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Input: output from 03_batch_inference.ipynb\n",
    "PATIENT_FILE = '/kaggle/input/loopguard-ai-outputs/patients_with_ai_final.json'\n",
    "ADAPTER_DIR  = '/kaggle/input/medgemma-loopguard-v2/transformers/default/1'\n",
    "BASE_MODEL   = 'google/medgemma-1.5-4b-it'\n",
    "OUTPUT_FILE  = '/kaggle/working/patients_with_ai_final.json'  # overwrite with enriched version\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "BNB_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    ")\n",
    "\n",
    "print('âœ… Config ready')\n",
    "print(f'   VRAM: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated())/1e9:.2f} GB free')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Load patients_with_ai_final.json\n",
    "# This is the output from 03_batch_inference â€” already has ai_analysis\n",
    "# ============================================================\n",
    "with open(PATIENT_FILE) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Handle both top-level formats\n",
    "if 'patient_scenarios' in data:\n",
    "    patients = data['patient_scenarios']\n",
    "elif isinstance(data, list):\n",
    "    patients = data\n",
    "else:\n",
    "    patients = list(data.values())\n",
    "\n",
    "print(f'âœ… Loaded {len(patients)} patients')\n",
    "print(f'   Keys: {list(patients[0].keys())}')\n",
    "print(f'   ai_analysis present: {\"ai_analysis\" in patients[0]}')\n",
    "if 'ai_analysis' in patients[0]:\n",
    "    print(f'   ai_analysis keys: {list(patients[0][\"ai_analysis\"].keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Agent 1 â€” Rule-based Pre-validator (no GPU needed)\n",
    "# Runs BEFORE model loading â€” fast, pure Python\n",
    "# ============================================================\n",
    "\n",
    "MEDICAL_KEYWORDS = [\n",
    "    'patient', 'presents', 'diagnosis', 'assessment', 'plan',\n",
    "    'history', 'symptoms', 'ordered', 'pmh', 'vitals', 'exam',\n",
    "    'subjective', 'objective', 'labs', 'impression'\n",
    "]\n",
    "\n",
    "REQUIRED_FIELDS = [\n",
    "    'primary_hypothesis', 'differential_diagnoses', 'key_supporting_evidence',\n",
    "    'urgency_level', 'tests_ordered', 'clinical_reasoning'\n",
    "]\n",
    "\n",
    "def agent1_validate(patient: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Rule-based pre-validator.\n",
    "    Checks: note quality, required field presence, urgency validity.\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    note_text = patient['clinical_note']['text']\n",
    "    ai = patient.get('ai_analysis', {})\n",
    "\n",
    "    # Check 1: Note length\n",
    "    if len(note_text) < 100:\n",
    "        issues.append('Note too short for reliable extraction (<100 chars)')\n",
    "\n",
    "    # Check 2: Clinical content present\n",
    "    note_lower = note_text.lower()\n",
    "    if not any(kw in note_lower for kw in MEDICAL_KEYWORDS):\n",
    "        issues.append('No clinical content detected in note')\n",
    "\n",
    "    # Check 3: All 6 fields populated in ai_analysis\n",
    "    missing = [f for f in REQUIRED_FIELDS if not ai.get(f)]\n",
    "    if missing:\n",
    "        issues.append(f'Missing fields in extraction: {missing}')\n",
    "\n",
    "    # Check 4: Urgency is valid\n",
    "    urgency = ai.get('urgency_level', '')\n",
    "    if urgency not in ('high', 'medium', 'low'):\n",
    "        issues.append(f'Invalid urgency value: \"{urgency}\"')\n",
    "\n",
    "    # Check 5: Primary hypothesis not a placeholder\n",
    "    hypothesis = ai.get('primary_hypothesis', '')\n",
    "    if len(hypothesis) < 5 or hypothesis.lower() in ('unknown', 'none', 'n/a', 'main diagnosis'):\n",
    "        issues.append('Primary hypothesis appears to be placeholder or empty')\n",
    "\n",
    "    return {\n",
    "        'passed': len(issues) == 0,\n",
    "        'issues': issues\n",
    "    }\n",
    "\n",
    "\n",
    "# Run Agent 1 on all patients\n",
    "print('Running Agent 1 (rule-based validator)...\\n')\n",
    "a1_results = {}\n",
    "all_passed = 0\n",
    "\n",
    "for p in patients:\n",
    "    pid = p['patient_id']\n",
    "    result = agent1_validate(p)\n",
    "    a1_results[pid] = result\n",
    "    tick = 'âœ…' if result['passed'] else 'âš ï¸'\n",
    "    issues_str = '' if result['passed'] else f\" â†’ {result['issues']}\"\n",
    "    print(f'  {tick} {pid}{issues_str}')\n",
    "    if result['passed']:\n",
    "        all_passed += 1\n",
    "\n",
    "print(f'\\nAgent 1: {all_passed}/20 passed validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Load Models\n",
    "# Base MedGemma â†’ Agents 3 & 4\n",
    "# Fine-tuned v2  â†’ Agent 2 (re-extraction if Agent 1 failed)\n",
    "# ============================================================\n",
    "print('Loading base MedGemma (for Agents 3 & 4)...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "base_model.eval()\n",
    "print(f'âœ… Base MedGemma loaded | VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB')\n",
    "\n",
    "print('\\nLoading LoopGuard v2 adapter (for Agent 2 re-extraction)...')\n",
    "ft_model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
    "ft_model.eval()\n",
    "print(f'âœ… LoopGuard v2 ready | VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Agent 2 â€” Re-extraction for failed Agent 1 cases\n",
    "# Uses fine-tuned LoopGuard v2\n",
    "# If Agent 1 passed â†’ skip (use existing ai_analysis)\n",
    "# If Agent 1 failed â†’ re-run inference\n",
    "# ============================================================\n",
    "\n",
    "FIELD_PATTERNS = [\n",
    "    ('primary_hypothesis',      r'PRIMARY HYPOTHESIS:'),\n",
    "    ('differential_diagnoses',  r'DIFFERENTIAL DIAGNOSES:'),\n",
    "    ('key_supporting_evidence', r'KEY SUPPORTING EVIDENCE:'),\n",
    "    ('urgency_level',           r'URGENCY LEVEL:'),\n",
    "    ('tests_ordered',           r'TESTS ORDERED:'),\n",
    "    ('clinical_reasoning',      r'CLINICAL REASONING:'),\n",
    "]\n",
    "FIELD_LABEL_RE = (\n",
    "    r'(?:PRIMARY HYPOTHESIS|DIFFERENTIAL DIAGNOSES|KEY SUPPORTING EVIDENCE|'\n",
    "    r'URGENCY LEVEL|TESTS ORDERED|CLINICAL REASONING):'\n",
    ")\n",
    "\n",
    "def decode_output(raw: str) -> str:\n",
    "    if 'model\\n' in raw:\n",
    "        return raw.split('model\\n')[-1].strip()\n",
    "    return raw.strip()\n",
    "\n",
    "def parse_fields(text: str) -> dict:\n",
    "    clean = re.sub(r'\\*\\*', '', text)\n",
    "    result = {}\n",
    "    for field_key, pattern in FIELD_PATTERNS:\n",
    "        m = re.search(pattern, clean, re.IGNORECASE)\n",
    "        if m:\n",
    "            start = m.end()\n",
    "            next_m = re.search(FIELD_LABEL_RE, clean[start:], re.IGNORECASE)\n",
    "            end = start + next_m.start() if next_m else start + 800\n",
    "            result[field_key] = clean[start:end].strip().strip('[]')\n",
    "        else:\n",
    "            result[field_key] = ''\n",
    "    u = result.get('urgency_level', '').lower()\n",
    "    result['urgency_level'] = 'high' if 'high' in u else ('low' if 'low' in u else 'medium')\n",
    "    return result\n",
    "\n",
    "def agent2_extract(note_text: str, model) -> dict:\n",
    "    prompt = (\n",
    "        '<start_of_turn>user\\n'\n",
    "        'Extract diagnostic information from this clinical note.\\n\\n'\n",
    "        f'Clinical Note:\\n{note_text}\\n\\n'\n",
    "        'Output ONLY these 6 fields:\\n'\n",
    "        'PRIMARY HYPOTHESIS: [main diagnosis]\\n'\n",
    "        'DIFFERENTIAL DIAGNOSES: [comma-separated alternatives]\\n'\n",
    "        'KEY SUPPORTING EVIDENCE: [comma-separated findings]\\n'\n",
    "        'URGENCY LEVEL: [high/medium/low]\\n'\n",
    "        'TESTS ORDERED: [comma-separated tests]\\n'\n",
    "        'CLINICAL REASONING: [brief explanation]'\n",
    "        '<end_of_turn>\\n<start_of_turn>model\\n'\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1024).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs, max_new_tokens=600, do_sample=False,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "        )\n",
    "    raw = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return parse_fields(decode_output(raw))\n",
    "\n",
    "\n",
    "print('Agent 2 â€” re-extraction for Agent 1 failures...\\n')\n",
    "a2_rerun_count = 0\n",
    "\n",
    "for p in patients:\n",
    "    pid = p['patient_id']\n",
    "    if not a1_results[pid]['passed']:\n",
    "        print(f'  Re-extracting {pid}...', end=' ')\n",
    "        note_text = p['clinical_note']['text']\n",
    "        new_extraction = agent2_extract(note_text, ft_model)\n",
    "        p['ai_analysis'].update(new_extraction)\n",
    "        a2_rerun_count += 1\n",
    "        fields_ok = sum(1 for v in new_extraction.values() if v)\n",
    "        print(f'âœ… {fields_ok}/6 fields')\n",
    "\n",
    "if a2_rerun_count == 0:\n",
    "    print('  All 20 passed Agent 1 â€” no re-extraction needed âœ…')\n",
    "else:\n",
    "    print(f'\\nAgent 2: re-extracted {a2_rerun_count} patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Agent 3 â€” Quality Checker (base MedGemma)\n",
    "# Reviews each extraction: urgency match, logical reasoning,\n",
    "# plausible differentials. Outputs PASS or FAIL: [reason]\n",
    "# ============================================================\n",
    "\n",
    "AGENT3_PROMPT = \"\"\"You are a medical quality reviewer checking an AI-generated diagnostic extraction.\n",
    "\n",
    "Review this extraction against the original clinical note and check:\n",
    "1. Does the urgency level match the clinical severity?\n",
    "2. Does the reasoning logically follow from the key evidence?\n",
    "3. Are the differential diagnoses clinically plausible?\n",
    "4. Is the primary hypothesis consistent with the presented symptoms?\n",
    "\n",
    "Clinical Note:\n",
    "{note}\n",
    "\n",
    "AI Extraction:\n",
    "PRIMARY HYPOTHESIS: {primary_hypothesis}\n",
    "DIFFERENTIAL DIAGNOSES: {differential_diagnoses}\n",
    "KEY SUPPORTING EVIDENCE: {key_supporting_evidence}\n",
    "URGENCY LEVEL: {urgency_level}\n",
    "TESTS ORDERED: {tests_ordered}\n",
    "CLINICAL REASONING: {clinical_reasoning}\n",
    "\n",
    "Output exactly one of:\n",
    "PASS\n",
    "FAIL: [specific issue found]\"\"\"\n",
    "\n",
    "\n",
    "def agent3_review(note_text: str, ai: dict, model) -> dict:\n",
    "    prompt_text = AGENT3_PROMPT.format(\n",
    "        note=note_text[:800],  # truncate long notes\n",
    "        primary_hypothesis=ai.get('primary_hypothesis', ''),\n",
    "        differential_diagnoses=ai.get('differential_diagnoses', ''),\n",
    "        key_supporting_evidence=ai.get('key_supporting_evidence', ''),\n",
    "        urgency_level=ai.get('urgency_level', ''),\n",
    "        tests_ordered=ai.get('tests_ordered', ''),\n",
    "        clinical_reasoning=ai.get('clinical_reasoning', ''),\n",
    "    )\n",
    "    prompt = (\n",
    "        f'<start_of_turn>user\\n{prompt_text}<end_of_turn>\\n<start_of_turn>model\\n'\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1536).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs, max_new_tokens=80, do_sample=False,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "        )\n",
    "    raw = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    generated = decode_output(raw)\n",
    "\n",
    "    # Parse PASS / FAIL\n",
    "    gen_upper = generated.strip().upper()\n",
    "    if gen_upper.startswith('PASS'):\n",
    "        return {'passed': True, 'issue': None, 'raw': generated}\n",
    "    elif 'FAIL' in gen_upper:\n",
    "        # Extract reason after 'FAIL:'\n",
    "        m = re.search(r'FAIL[:\\s]+(.+)', generated, re.IGNORECASE | re.DOTALL)\n",
    "        issue = m.group(1).strip()[:200] if m else 'Quality check failed'\n",
    "        return {'passed': False, 'issue': issue, 'raw': generated}\n",
    "    else:\n",
    "        # Ambiguous output â€” treat as pass but note it\n",
    "        return {'passed': True, 'issue': None, 'raw': generated}\n",
    "\n",
    "\n",
    "print('Running Agent 3 (quality checker) on all 20 patients...\\n')\n",
    "a3_results = {}\n",
    "\n",
    "for i, p in enumerate(patients):\n",
    "    pid = p['patient_id']\n",
    "    note_text = p['clinical_note']['text']\n",
    "    ai = p['ai_analysis']\n",
    "\n",
    "    print(f'  [{i+1:02d}/20] {pid}...', end=' ', flush=True)\n",
    "    result = agent3_review(note_text, ai, base_model)\n",
    "    a3_results[pid] = result\n",
    "\n",
    "    tick = 'âœ… PASS' if result['passed'] else f'âš ï¸  FAIL: {result[\"issue\"][:60]}'\n",
    "    print(tick)\n",
    "\n",
    "passed_count = sum(1 for r in a3_results.values() if r['passed'])\n",
    "print(f'\\nAgent 3: {passed_count}/20 passed quality review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Agent 4 â€” Confidence Scorer (base MedGemma)\n",
    "# Rates diagnostic confidence 1-10\n",
    "# Flags atypical presentations, conflicting findings, rare diagnoses\n",
    "# ============================================================\n",
    "\n",
    "AGENT4_PROMPT = \"\"\"You are assessing diagnostic confidence for an AI-generated clinical extraction.\n",
    "\n",
    "Rate the confidence that the primary hypothesis is correct, and flag if review is needed.\n",
    "Flag if: atypical presentation, conflicting findings, rare diagnosis, or critical urgency mismatch.\n",
    "\n",
    "Clinical Note (summary):\n",
    "{note}\n",
    "\n",
    "AI Extraction:\n",
    "PRIMARY HYPOTHESIS: {primary_hypothesis}\n",
    "URGENCY LEVEL: {urgency_level}\n",
    "KEY SUPPORTING EVIDENCE: {key_supporting_evidence}\n",
    "CLINICAL REASONING: {clinical_reasoning}\n",
    "\n",
    "Output exactly:\n",
    "CONFIDENCE: [1-10]\n",
    "FLAG: [yes/no]\n",
    "REASON: [one sentence if flagged, none if not]\"\"\"\n",
    "\n",
    "\n",
    "def agent4_score(note_text: str, ai: dict, model) -> dict:\n",
    "    prompt_text = AGENT4_PROMPT.format(\n",
    "        note=note_text[:600],\n",
    "        primary_hypothesis=ai.get('primary_hypothesis', ''),\n",
    "        urgency_level=ai.get('urgency_level', ''),\n",
    "        key_supporting_evidence=ai.get('key_supporting_evidence', '')[:200],\n",
    "        clinical_reasoning=ai.get('clinical_reasoning', '')[:200],\n",
    "    )\n",
    "    prompt = f'<start_of_turn>user\\n{prompt_text}<end_of_turn>\\n<start_of_turn>model\\n'\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1536).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs, max_new_tokens=60, do_sample=False,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "        )\n",
    "    raw = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    generated = decode_output(raw)\n",
    "\n",
    "    # Parse CONFIDENCE, FLAG, REASON\n",
    "    confidence = 7  # default\n",
    "    flagged = False\n",
    "    reason = 'none'\n",
    "\n",
    "    m_conf = re.search(r'CONFIDENCE[:\\s]+([0-9]+(?:\\.[0-9]+)?)', generated, re.IGNORECASE)\n",
    "    if m_conf:\n",
    "        try:\n",
    "            confidence = min(10, max(1, int(float(m_conf.group(1)))))\n",
    "        except:\n",
    "            confidence = 7\n",
    "\n",
    "    m_flag = re.search(r'FLAG[:\\s]+(yes|no)', generated, re.IGNORECASE)\n",
    "    if m_flag:\n",
    "        flagged = m_flag.group(1).lower() == 'yes'\n",
    "\n",
    "    m_reason = re.search(r'REASON[:\\s]+(.+)', generated, re.IGNORECASE | re.DOTALL)\n",
    "    if m_reason:\n",
    "        reason_text = m_reason.group(1).strip()[:200]\n",
    "        reason = reason_text if reason_text.lower() != 'none' else 'none'\n",
    "\n",
    "    return {\n",
    "        'confidence': confidence,\n",
    "        'flagged': flagged,\n",
    "        'reason': reason,\n",
    "        'raw': generated\n",
    "    }\n",
    "\n",
    "\n",
    "print('Running Agent 4 (confidence scorer) on all 20 patients...\\n')\n",
    "a4_results = {}\n",
    "\n",
    "for i, p in enumerate(patients):\n",
    "    pid = p['patient_id']\n",
    "    note_text = p['clinical_note']['text']\n",
    "    ai = p['ai_analysis']\n",
    "\n",
    "    print(f'  [{i+1:02d}/20] {pid}...', end=' ', flush=True)\n",
    "    result = agent4_score(note_text, ai, base_model)\n",
    "    a4_results[pid] = result\n",
    "\n",
    "    flag_str = f' ðŸš© {result[\"reason\"][:50]}' if result['flagged'] else ''\n",
    "    print(f'confidence={result[\"confidence\"]}/10 flag={result[\"flagged\"]}{flag_str}')\n",
    "\n",
    "flagged_count = sum(1 for r in a4_results.values() if r['flagged'])\n",
    "avg_conf = sum(r['confidence'] for r in a4_results.values()) / len(a4_results)\n",
    "print(f'\\nAgent 4: avg confidence={avg_conf:.1f}/10 | flagged={flagged_count}/20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Merge Agent Results + Save Final JSON\n",
    "# Each patient now has:\n",
    "#   ai_analysis.agent_validation  â†’ Agent 1 result\n",
    "#   ai_analysis.agent_quality     â†’ Agent 3 result\n",
    "#   ai_analysis.agent_confidence  â†’ Agent 4 score\n",
    "#   ai_analysis.agent_review_flag â†’ True if any agent raised concern\n",
    "# ============================================================\n",
    "\n",
    "for p in patients:\n",
    "    pid = p['patient_id']\n",
    "    ai = p['ai_analysis']\n",
    "\n",
    "    a1 = a1_results[pid]\n",
    "    a3 = a3_results[pid]\n",
    "    a4 = a4_results[pid]\n",
    "\n",
    "    # Master flag: any agent raised a concern\n",
    "    review_flag = (\n",
    "        not a1['passed'] or\n",
    "        not a3['passed'] or\n",
    "        a4['flagged'] or\n",
    "        a4['confidence'] <= 5\n",
    "    )\n",
    "\n",
    "    ai['agent_validation'] = {\n",
    "        'passed': a1['passed'],\n",
    "        'issues': a1['issues'],\n",
    "    }\n",
    "    ai['agent_quality'] = {\n",
    "        'passed': a3['passed'],\n",
    "        'issue': a3['issue'],\n",
    "    }\n",
    "    ai['agent_confidence'] = a4['confidence']\n",
    "    ai['agent_flagged'] = a4['flagged']\n",
    "    ai['agent_flag_reason'] = a4['reason']\n",
    "    ai['agent_review_flag'] = review_flag\n",
    "\n",
    "    # Clean up internal debug fields before saving\n",
    "    ai.pop('_raw_output', None)\n",
    "    ai.pop('_fields_present', None)\n",
    "\n",
    "\n",
    "# Save\n",
    "output = {'patient_scenarios': patients}\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "size_kb = len(json.dumps(output)) / 1024\n",
    "print(f'âœ… Saved â†’ {OUTPUT_FILE} ({size_kb:.1f} KB)')\n",
    "print(f'   {len(patients)} patients with full agent metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Pipeline Summary Report\n",
    "# ============================================================\n",
    "print('=== LOOPGUARD AGENTIC PIPELINE REPORT ===')\n",
    "print(f'{\"Patient\":<8} {\"A1\":<6} {\"A3\":<6} {\"Conf\":<6} {\"Flag\":<6} {\"Urgency\"}')\n",
    "print('-' * 55)\n",
    "\n",
    "for p in patients:\n",
    "    pid = p['patient_id']\n",
    "    ai = p['ai_analysis']\n",
    "    a1_ok = 'âœ…' if ai['agent_validation']['passed'] else 'âš ï¸'\n",
    "    a3_ok = 'âœ…' if ai['agent_quality']['passed'] else 'âš ï¸'\n",
    "    conf = ai['agent_confidence']\n",
    "    flag = 'ðŸš©' if ai['agent_review_flag'] else '  '\n",
    "    urg = ai['urgency_level']\n",
    "    print(f'{pid:<8} {a1_ok:<6} {a3_ok:<6} {conf:<6} {flag:<6} {urg}')\n",
    "\n",
    "print()\n",
    "total_flagged = sum(1 for p in patients if p['ai_analysis']['agent_review_flag'])\n",
    "avg_conf = sum(p['ai_analysis']['agent_confidence'] for p in patients) / len(patients)\n",
    "a1_pass = sum(1 for p in patients if p['ai_analysis']['agent_validation']['passed'])\n",
    "a3_pass = sum(1 for p in patients if p['ai_analysis']['agent_quality']['passed'])\n",
    "\n",
    "print(f'Agent 1 (validation):  {a1_pass}/20 passed')\n",
    "print(f'Agent 3 (quality):     {a3_pass}/20 passed')\n",
    "print(f'Agent 4 (confidence):  avg {avg_conf:.1f}/10')\n",
    "print(f'Flagged for review:    {total_flagged}/20')\n",
    "print(f'\\nâœ… Pipeline complete. Download patients_with_ai_final.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
