{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# LoopGuard ‚Äî 7-Model Comparison Evaluation\n\n**Purpose:** Prove that MedGemma + task-specific fine-tuning is the correct approach for privacy-sensitive clinical NLP. Covers Criterion 1 (HAI-DEF Use, 20%) and Criterion 4 (Product Feasibility, 20%).\n\n## Models (all ‚â§7B, all open-source, all locally deployable)\n\n| # | Model | Size | Type | Narrative role |\n|---|-------|------|------|----------------|\n| 1 | `google/gemma-2-2b-it` | 2B | General | Minimum baseline |\n| 2 | `meta-llama/Llama-3.2-3B-Instruct` | 3B | General | Best popular 3B |\n| 3 | `BioMistral/BioMistral-7B` | 7B | Medical (PubMed) | Literature ‚â† clinical notes |\n| 4 | `Qwen/Qwen2.5-7B-Instruct` | 7B | General | Best general 7B |\n| 5 | `deepseek-ai/DeepSeek-R1-Distill-Qwen-7B` | 7B | Reasoning | Reasoning ‚â† domain knowledge |\n| 6 | `google/medgemma-1.5-4b-it` | 4B | Medical | Right domain, needs fine-tuning |\n| 7 | **LoopGuard v2** | 4B | Medical FT | ‚úÖ Right domain + right task |\n\n## Chat template notes (from official docs)\n- **Gemma 2**: `<start_of_turn>user/model` ‚Äî via `apply_chat_template()`\n- **Llama 3.2**: `<|begin_of_text|><|start_header_id|>` ‚Äî via `apply_chat_template()`\n- **BioMistral**: `[INST]...[/INST]` (Mistral v0.1 format) ‚Äî **no system message** supported\n- **Qwen2.5**: ChatML `<|im_start|>` ‚Äî via `apply_chat_template()`\n- **DeepSeek-R1 Distill**: ChatML (Qwen2.5 tokenizer) ‚Äî via `apply_chat_template()`, **no system prompt**, produces `<think>` tokens to strip\n- **MedGemma / LoopGuard**: `<start_of_turn>user/model` ‚Äî same as Gemma 2\n\n## Workflow\n1. **Cell 1** ‚Äî Install + restart kernel\n2. **Cell 2** ‚Äî Imports + shared config\n3. **Cell 3** ‚Äî Test notes + scoring functions\n4. **Cells 4‚Äì10** ‚Äî Run each model (load ‚Üí infer ‚Üí free VRAM)\n5. **Cell 11** ‚Äî Score all results + print table\n6. **Cell 12** ‚Äî Save bar chart PNG\n7. **Cell 13** ‚Äî Print writeup-ready text",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 1: Install Dependencies ‚Äî restart kernel after\n# ============================================================\n!pip uninstall -y -q transformers peft bitsandbytes accelerate\n!pip install -q transformers>=4.47.0\n!pip install -q peft>=0.13.0\n!pip install -q accelerate>=0.34.0\n!pip install -q bitsandbytes>=0.46.1\n!pip install -q matplotlib\n\nprint(\"‚úÖ Done. ‚ö†Ô∏è RESTART KERNEL, then run from Cell 2.\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 2: Imports + Shared BnB Config\n# ============================================================\nimport torch\nimport json\nimport re\nimport os\nimport gc\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\nprint(f\"‚úÖ Imports OK\")\nprint(f\"   PyTorch: {torch.__version__}\")\nprint(f\"   CUDA: {torch.cuda.is_available()} | GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'none'}\")\nprint(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n# Shared 4-bit quantization config used for all models\nBNB_CONFIG = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\ndef free_vram(model=None, tokenizer=None):\n    \"\"\"Delete model/tokenizer objects and free GPU memory.\"\"\"\n    if model is not None:\n        del model\n    if tokenizer is not None:\n        del tokenizer\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(f\"   VRAM freed ‚Üí {torch.cuda.memory_allocated()/1e9:.2f} GB in use\")\n\nprint(\"\\n‚úÖ Shared config ready\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 3: Test Notes, Scoring Functions, Results Store\n# ============================================================\n\nTEST_NOTES = [\n    {\"id\": \"N01\", \"specialty\": \"Oncology\",       \"urgency_gt\": \"high\",\n     \"note\": \"58-year-old female presents with 3-month history of intermittent abdominal bloating and early satiety. Reports feeling full after eating small amounts. Postmenopausal (LMP 7 years ago). Abdomen: mildly distended, positive fluid wave. CA-125 ordered. Assessment: Rule out ovarian malignancy. Plan: CA-125, transvaginal ultrasound, follow-up 6 weeks.\"},\n    {\"id\": \"N02\", \"specialty\": \"Cardiology\",     \"urgency_gt\": \"high\",\n     \"note\": \"58-year-old male presents with sudden onset crushing substernal chest pressure, 8/10, radiating to left arm and jaw. Started 45 minutes ago while shoveling snow. Diaphoresis, nausea, dyspnea. PMH: Hypertension, hyperlipidemia, smoking 1 PPD x 30 years. BP 160/95, HR 102. EKG shows ST elevations 2-3mm in leads II, III, aVF. Assessment: Acute inferior STEMI. Plan: Activate cath lab, aspirin, heparin.\"},\n    {\"id\": \"N03\", \"specialty\": \"Pulmonology\",    \"urgency_gt\": \"high\",\n     \"note\": \"42-year-old female with sudden onset dyspnea and right-sided pleuritic chest pain. 6-hour flight from Europe yesterday. On oral contraceptives. BP 108/68, HR 118, RR 28, SpO2 88% on RA. Right leg with 2+ edema and calf tenderness. EKG: S1Q3T3 pattern. Assessment: High probability pulmonary embolism. Plan: CTPA, D-dimer, anticoagulation.\"},\n    {\"id\": \"N04\", \"specialty\": \"Neurology\",      \"urgency_gt\": \"high\",\n     \"note\": \"67-year-old male presents with sudden onset left-sided weakness and facial droop, onset 2 hours ago. PMH: Atrial fibrillation, hypertension, not on anticoagulation. BP 188/104, HR 78 irregular. Neuro: Left hemiparesis, left facial droop, dysarthria. Assessment: Acute ischemic stroke. Plan: Stat CT head, labs, neurology consult, tPA evaluation.\"},\n    {\"id\": \"N05\", \"specialty\": \"Sepsis\",         \"urgency_gt\": \"high\",\n     \"note\": \"72-year-old female presents with fever 102.8F, confusion, and burning urination x 3 days. PMH: Type 2 diabetes, recurrent UTIs. BP 88/52, HR 118, RR 24, SpO2 94%. Exam: Altered mental status, CVA tenderness. Labs: WBC 18.4, lactate 3.2. Assessment: Urosepsis. Plan: Blood cultures x2, urine culture, IV antibiotics, 30cc/kg fluid bolus.\"},\n    {\"id\": \"N06\", \"specialty\": \"Endocrinology\",  \"urgency_gt\": \"medium\",\n     \"note\": \"45-year-old female with 6-month history of fatigue, weight gain (15 lbs), cold intolerance, constipation, and dry skin. Hair thinning noted. No family history of thyroid disease. Vitals normal. Exam: Delayed DTR relaxation, mild facial puffiness, dry skin. Assessment: Hypothyroidism. Plan: TSH, free T4, CBC, CMP.\"},\n    {\"id\": \"N07\", \"specialty\": \"Hematology\",     \"urgency_gt\": \"medium\",\n     \"note\": \"34-year-old female presents with 3-month history of fatigue, exertional dyspnea, and palpitations. Heavy menstrual periods. Vitals: BP 110/70, HR 96. Exam: Pale conjunctivae, tachycardia. Assessment: Iron deficiency anemia secondary to menorrhagia. Plan: CBC, iron studies, ferritin, reticulocyte count, gynecology referral.\"},\n    {\"id\": \"N08\", \"specialty\": \"Gastroenterology\",\"urgency_gt\": \"medium\",\n     \"note\": \"52-year-old male with 2-month history of right upper quadrant pain, jaundice, and unintentional weight loss of 18 lbs. Pruritus. PMH: Chronic alcohol use. Vitals: BP 118/76, HR 88. Exam: Jaundice, RUQ tenderness, palpable gallbladder. Assessment: Obstructive jaundice, rule out cholangiocarcinoma or pancreatic head mass. Plan: LFTs, bilirubin, CT abdomen/pelvis, MRCP.\"},\n    {\"id\": \"N09\", \"specialty\": \"Nephrology\",     \"urgency_gt\": \"medium\",\n     \"note\": \"61-year-old male with type 2 diabetes and hypertension presenting for routine follow-up. Creatinine has risen from 1.2 to 2.1 over 6 months. Foamy urine noted. BP 158/96 despite lisinopril 20mg. Exam: 1+ bilateral lower extremity edema. Assessment: Progressive diabetic nephropathy, stage 3b CKD. Plan: Nephrology referral, urine albumin/creatinine, renal ultrasound.\"},\n    {\"id\": \"N10\", \"specialty\": \"Preventive\",     \"urgency_gt\": \"low\",\n     \"note\": \"45-year-old male presents for annual wellness exam. No complaints. PMH: None. Nonsmoker, social drinker. BMI 27. BP 124/78, HR 68. Exam normal. Assessment: Healthy male due for age-appropriate screening. Plan: Lipid panel, fasting glucose, colorectal cancer screening discussion, HIV screen, tetanus booster.\"},\n]\n\nREQUIRED_FIELDS = [\n    \"PRIMARY HYPOTHESIS\",\n    \"DIFFERENTIAL DIAGNOSES\",\n    \"KEY SUPPORTING EVIDENCE\",\n    \"URGENCY LEVEL\",\n    \"TESTS ORDERED\",\n    \"CLINICAL REASONING\",\n]\n\n# Storage for all model results\nALL_RESULTS = {}\n\ndef extract_urgency(text):\n    match = re.search(r'urgency level[:\\s]+([\\w]+)', text.lower())\n    if match:\n        val = match.group(1).strip()\n        if val in (\"high\", \"medium\", \"low\"):\n            return val\n    # Fallback: look for standalone urgency words\n    for u in [\"high\", \"medium\", \"low\"]:\n        if re.search(rf'\\burgency\\b.*\\b{u}\\b', text.lower()):\n            return u\n    return \"unknown\"\n\ndef score_output(generated_text, ground_truth_urgency):\n    text_upper = generated_text.upper()\n    fields_found = sum(1 for f in REQUIRED_FIELDS if f in text_upper)\n    urgency_detected = extract_urgency(generated_text)\n    return {\n        \"fields_present\": fields_found,\n        \"completeness_pct\": round(100 * fields_found / len(REQUIRED_FIELDS)),\n        \"valid_structure\": fields_found >= 5,\n        \"urgency_detected\": urgency_detected,\n        \"urgency_match\": urgency_detected == ground_truth_urgency.lower(),\n    }\n\ndef summarize_results(results, model_display_name):\n    avg_completeness = sum(r['completeness_pct'] for r in results) / len(results)\n    pct_valid = 100 * sum(1 for r in results if r['valid_structure']) / len(results)\n    pct_urgency = 100 * sum(1 for r in results if r['urgency_match']) / len(results)\n    return {\n        \"model\": model_display_name,\n        \"avg_completeness\": round(avg_completeness, 1),\n        \"pct_valid_structure\": round(pct_valid, 1),\n        \"pct_urgency_correct\": round(pct_urgency, 1),\n        \"per_note\": results,\n    }\n\nprint(f\"‚úÖ {len(TEST_NOTES)} test notes ready\")\nprint(f\"   Urgency breakdown: high={sum(1 for n in TEST_NOTES if n['urgency_gt']=='high')}, medium={sum(1 for n in TEST_NOTES if n['urgency_gt']=='medium')}, low={sum(1 for n in TEST_NOTES if n['urgency_gt']=='low')}\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 4: MODEL 1 ‚Äî Gemma 2 2B-it\n# Chat template: <start_of_turn>user/model  (via apply_chat_template)\n# ~10 min\n# ============================================================\nMODEL_ID = \"google/gemma-2-2b-it\"\nDISPLAY_NAME = \"Gemma 2 2B\\n(general, no medical)\"\nprint(f\"\\nüî¨ [{1}/7] {MODEL_ID}\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, quantization_config=BNB_CONFIG, device_map=\"auto\",\n    trust_remote_code=True, dtype=torch.bfloat16)\nmodel.eval()\nprint(f\"‚úÖ Loaded | VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\nresults = []\nfor i, note in enumerate(TEST_NOTES):\n    messages = [{\"role\": \"user\", \"content\": (\n        \"Extract diagnostic information from this clinical note.\\n\\n\"\n        f\"Clinical Note:\\n{note['note']}\\n\\n\"\n        \"Output ONLY these 6 fields:\\n\"\n        \"PRIMARY HYPOTHESIS: [diagnosis]\\n\"\n        \"DIFFERENTIAL DIAGNOSES: [comma-separated list]\\n\"\n        \"KEY SUPPORTING EVIDENCE: [comma-separated list]\\n\"\n        \"URGENCY LEVEL: [high/medium/low]\\n\"\n        \"TESTS ORDERED: [comma-separated list]\\n\"\n        \"CLINICAL REASONING: [brief reasoning]\"\n    )}]\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=768).to(model.device)\n    with torch.no_grad():\n        out = model.generate(**inputs, max_new_tokens=400, do_sample=False,\n                             repetition_penalty=1.1, pad_token_id=tokenizer.eos_token_id)\n    response = tokenizer.decode(out[0], skip_special_tokens=True)\n    # Extract model turn only\n    if \"<start_of_turn>model\" in response:\n        generated = response.split(\"<start_of_turn>model\")[-1].split(\"<end_of_turn>\")[0].strip()\n    else:\n        generated = response[len(prompt):].strip()\n    scores = score_output(generated, note[\"urgency_gt\"])\n    results.append({\"note_id\": note[\"id\"], **scores})\n    print(f\"  [{i+1}/10] {note['id']} fields={scores['fields_present']}/6 urgency_match={scores['urgency_match']}\")\n\nALL_RESULTS[DISPLAY_NAME] = summarize_results(results, DISPLAY_NAME)\nfree_vram(model, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 5: MODEL 2 ‚Äî Llama 3.2 3B Instruct\n# Chat template: <|begin_of_text|><|start_header_id|>  (via apply_chat_template)\n# Note: Llama requires HF token ‚Äî make sure internet is enabled and you're logged in\n# ~10 min\n# ============================================================\nMODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\nDISPLAY_NAME = \"Llama 3.2 3B\\n(general, no medical)\"\nprint(f\"\\nüî¨ [{2}/7] {MODEL_ID}\")\nprint(\"   Note: Requires HF token. If it fails, run: from huggingface_hub import notebook_login; notebook_login()\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, quantization_config=BNB_CONFIG, device_map=\"auto\",\n    trust_remote_code=True, dtype=torch.bfloat16)\nmodel.eval()\nprint(f\"‚úÖ Loaded | VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\nresults = []\nfor i, note in enumerate(TEST_NOTES):\n    # Llama 3.2 supports system messages via apply_chat_template\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a clinical documentation assistant. Output ONLY the structured format requested. No additional text.\"},\n        {\"role\": \"user\", \"content\": (\n            \"Extract diagnostic information from this clinical note.\\n\\n\"\n            f\"Clinical Note:\\n{note['note']}\\n\\n\"\n            \"Output ONLY these 6 fields:\\n\"\n            \"PRIMARY HYPOTHESIS: [diagnosis]\\n\"\n            \"DIFFERENTIAL DIAGNOSES: [comma-separated list]\\n\"\n            \"KEY SUPPORTING EVIDENCE: [comma-separated list]\\n\"\n            \"URGENCY LEVEL: [high/medium/low]\\n\"\n            \"TESTS ORDERED: [comma-separated list]\\n\"\n            \"CLINICAL REASONING: [brief reasoning]\"\n        )}\n    ]\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=768).to(model.device)\n    with torch.no_grad():\n        out = model.generate(**inputs, max_new_tokens=400, do_sample=False,\n                             repetition_penalty=1.1, pad_token_id=tokenizer.eos_token_id)\n    response = tokenizer.decode(out[0], skip_special_tokens=True)\n    # Extract assistant turn: appears after <|start_header_id|>assistant<|end_header_id|>\n    if \"assistant\" in response.lower():\n        parts = re.split(r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>', response, flags=re.IGNORECASE)\n        generated = parts[-1].strip() if len(parts) > 1 else response[len(prompt):].strip()\n    else:\n        generated = response[len(prompt):].strip()\n    # Strip any trailing eot tokens\n    generated = generated.replace(\"<|eot_id|>\", \"\").strip()\n    scores = score_output(generated, note[\"urgency_gt\"])\n    results.append({\"note_id\": note[\"id\"], **scores})\n    print(f\"  [{i+1}/10] {note['id']} fields={scores['fields_present']}/6 urgency_match={scores['urgency_match']}\")\n\nALL_RESULTS[DISPLAY_NAME] = summarize_results(results, DISPLAY_NAME)\nfree_vram(model, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 6: MODEL 3 ‚Äî BioMistral 7B\n# Chat template: [INST]...[/INST]  (Mistral v0.1 format)\n# IMPORTANT: BioMistral does NOT support system messages.\n# Per official docs: user content only, no system role.\n# ~12 min\n# ============================================================\nMODEL_ID = \"BioMistral/BioMistral-7B\"\nDISPLAY_NAME = \"BioMistral 7B\\n(medical, PubMed pretrain)\"\nprint(f\"\\nüî¨ [{3}/7] {MODEL_ID}\")\nprint(\"   Medical model pretrained on PubMed Central ‚Äî but no clinical note fine-tuning\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, quantization_config=BNB_CONFIG, device_map=\"auto\",\n    trust_remote_code=True, dtype=torch.bfloat16)\nmodel.eval()\nprint(f\"‚úÖ Loaded | VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\nresults = []\nfor i, note in enumerate(TEST_NOTES):\n    # BioMistral uses Mistral [INST] format, NO system message supported\n    # We use apply_chat_template which handles this correctly\n    messages = [{\"role\": \"user\", \"content\": (\n        \"Extract diagnostic information from this clinical note. \"\n        \"Output ONLY the 6 structured fields below, no other text.\\n\\n\"\n        f\"Clinical Note:\\n{note['note']}\\n\\n\"\n        \"PRIMARY HYPOTHESIS: [diagnosis]\\n\"\n        \"DIFFERENTIAL DIAGNOSES: [comma-separated list]\\n\"\n        \"KEY SUPPORTING EVIDENCE: [comma-separated list]\\n\"\n        \"URGENCY LEVEL: [high/medium/low]\\n\"\n        \"TESTS ORDERED: [comma-separated list]\\n\"\n        \"CLINICAL REASONING: [brief reasoning]\"\n    )}]\n    try:\n        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    except Exception:\n        # Fallback to manual Mistral format if apply_chat_template fails\n        prompt = f\"<s>[INST] {messages[0]['content']} [/INST]\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=768).to(model.device)\n    with torch.no_grad():\n        out = model.generate(**inputs, max_new_tokens=400, do_sample=False,\n                             repetition_penalty=1.1, pad_token_id=tokenizer.eos_token_id)\n    response = tokenizer.decode(out[0], skip_special_tokens=True)\n    # Extract content after [/INST]\n    if \"[/INST]\" in response:\n        generated = response.split(\"[/INST]\")[-1].strip()\n    else:\n        generated = response[len(prompt):].strip()\n    scores = score_output(generated, note[\"urgency_gt\"])\n    results.append({\"note_id\": note[\"id\"], **scores})\n    print(f\"  [{i+1}/10] {note['id']} fields={scores['fields_present']}/6 urgency_match={scores['urgency_match']}\")\n\nALL_RESULTS[DISPLAY_NAME] = summarize_results(results, DISPLAY_NAME)\nfree_vram(model, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 7: MODEL 4 ‚Äî Qwen2.5 7B Instruct\n# Chat template: ChatML <|im_start|>  (via apply_chat_template)\n# Arguably the strongest general <10B model currently\n# ~12 min\n# ============================================================\nMODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\nDISPLAY_NAME = \"Qwen2.5 7B\\n(general, best <10B)\"\nprint(f\"\\nüî¨ [{4}/7] {MODEL_ID}\")\nprint(\"   Strongest general open-source model under 10B ‚Äî still lacks medical domain knowledge\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, quantization_config=BNB_CONFIG, device_map=\"auto\",\n    trust_remote_code=True, dtype=torch.bfloat16)\nmodel.eval()\nprint(f\"‚úÖ Loaded | VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\nresults = []\nfor i, note in enumerate(TEST_NOTES):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a clinical documentation assistant. Output ONLY the structured format requested. No preamble, no explanation.\"},\n        {\"role\": \"user\", \"content\": (\n            \"Extract diagnostic information from this clinical note.\\n\\n\"\n            f\"Clinical Note:\\n{note['note']}\\n\\n\"\n            \"Output ONLY these 6 fields:\\n\"\n            \"PRIMARY HYPOTHESIS: [diagnosis]\\n\"\n            \"DIFFERENTIAL DIAGNOSES: [comma-separated list]\\n\"\n            \"KEY SUPPORTING EVIDENCE: [comma-separated list]\\n\"\n            \"URGENCY LEVEL: [high/medium/low]\\n\"\n            \"TESTS ORDERED: [comma-separated list]\\n\"\n            \"CLINICAL REASONING: [brief reasoning]\"\n        )}\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer([text], return_tensors=\"pt\", truncation=True, max_length=768).to(model.device)\n    with torch.no_grad():\n        out = model.generate(**inputs, max_new_tokens=400, do_sample=False,\n                             repetition_penalty=1.1)\n    generated = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n    scores = score_output(generated, note[\"urgency_gt\"])\n    results.append({\"note_id\": note[\"id\"], **scores})\n    print(f\"  [{i+1}/10] {note['id']} fields={scores['fields_present']}/6 urgency_match={scores['urgency_match']}\")\n\nALL_RESULTS[DISPLAY_NAME] = summarize_results(results, DISPLAY_NAME)\nfree_vram(model, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 8: MODEL 5 ‚Äî DeepSeek-R1 Distill Qwen 7B\n# Chat template: ChatML (Qwen2.5 tokenizer) via apply_chat_template\n# IMPORTANT per DeepSeek docs:\n#   - No system prompt ‚Äî all instructions in user turn\n#   - Produces <think>...</think> reasoning tokens ‚Äî must strip before scoring\n#   - Recommended temperature 0.6 (we use greedy for fairness)\n# ~15 min (generates think tokens before output)\n# ============================================================\nMODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\nDISPLAY_NAME = \"DeepSeek-R1 7B\\n(reasoning, no medical)\"\nprint(f\"\\nüî¨ [{5}/7] {MODEL_ID}\")\nprint(\"   Reasoning model ‚Äî will produce <think> tokens. We strip these before scoring.\")\nprint(\"   Per DeepSeek docs: no system prompt, instructions in user turn only.\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, quantization_config=BNB_CONFIG, device_map=\"auto\",\n    trust_remote_code=True, dtype=torch.bfloat16)\nmodel.eval()\nprint(f\"‚úÖ Loaded | VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\ndef strip_think_tokens(text):\n    \"\"\"Remove DeepSeek-R1 chain-of-thought <think>...</think> blocks.\"\"\"\n    # Remove think blocks\n    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n    return text.strip()\n\nresults = []\nfor i, note in enumerate(TEST_NOTES):\n    # DeepSeek-R1: NO system prompt per official recommendation\n    messages = [{\"role\": \"user\", \"content\": (\n        \"Extract diagnostic information from this clinical note. \"\n        \"Output ONLY the 6 structured fields below. Do not include any other text or reasoning.\\n\\n\"\n        f\"Clinical Note:\\n{note['note']}\\n\\n\"\n        \"PRIMARY HYPOTHESIS: [diagnosis]\\n\"\n        \"DIFFERENTIAL DIAGNOSES: [comma-separated list]\\n\"\n        \"KEY SUPPORTING EVIDENCE: [comma-separated list]\\n\"\n        \"URGENCY LEVEL: [high/medium/low]\\n\"\n        \"TESTS ORDERED: [comma-separated list]\\n\"\n        \"CLINICAL REASONING: [brief reasoning]\"\n    )}]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer([text], return_tensors=\"pt\", truncation=True, max_length=768).to(model.device)\n    with torch.no_grad():\n        out = model.generate(**inputs, max_new_tokens=600, do_sample=False,\n                             repetition_penalty=1.1)\n    raw = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n    # Strip <think> reasoning blocks before scoring\n    generated = strip_think_tokens(raw)\n    scores = score_output(generated, note[\"urgency_gt\"])\n    results.append({\"note_id\": note[\"id\"], **scores, \"had_think_tokens\": \"<think>\" in raw})\n    think_note = \" [had <think> tokens]\" if \"<think>\" in raw else \"\"\n    print(f\"  [{i+1}/10] {note['id']} fields={scores['fields_present']}/6 urgency_match={scores['urgency_match']}{think_note}\")\n\nALL_RESULTS[DISPLAY_NAME] = summarize_results(results, DISPLAY_NAME)\nfree_vram(model, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 9: MODEL 6 ‚Äî Base MedGemma 1.5 4B-it (zero-shot)\n# Chat template: <start_of_turn>user/model  (via apply_chat_template)\n# Same model as LoopGuard base ‚Äî proves fine-tuning adds value\n# ~10 min\n# ============================================================\nMODEL_ID = \"google/medgemma-1.5-4b-it\"\nDISPLAY_NAME = \"Base MedGemma 4B\\n(medical, zero-shot)\"\nprint(f\"\\nüî¨ [{6}/7] {MODEL_ID}\")\nprint(\"   Same base as LoopGuard ‚Äî proves task fine-tuning adds value on top of domain pretraining\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, quantization_config=BNB_CONFIG, device_map=\"auto\",\n    trust_remote_code=True, dtype=torch.bfloat16)\nmodel.eval()\nprint(f\"‚úÖ Loaded | VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\nresults = []\nfor i, note in enumerate(TEST_NOTES):\n    messages = [{\"role\": \"user\", \"content\": (\n        \"Extract diagnostic information from this clinical note.\\n\\n\"\n        f\"Clinical Note:\\n{note['note']}\\n\\n\"\n        \"Output ONLY these 6 fields:\\n\"\n        \"PRIMARY HYPOTHESIS: [diagnosis]\\n\"\n        \"DIFFERENTIAL DIAGNOSES: [comma-separated list]\\n\"\n        \"KEY SUPPORTING EVIDENCE: [comma-separated list]\\n\"\n        \"URGENCY LEVEL: [high/medium/low]\\n\"\n        \"TESTS ORDERED: [comma-separated list]\\n\"\n        \"CLINICAL REASONING: [brief reasoning]\"\n    )}]\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=768).to(model.device)\n    with torch.no_grad():\n        out = model.generate(**inputs, max_new_tokens=400, do_sample=False,\n                             repetition_penalty=1.1, pad_token_id=tokenizer.eos_token_id)\n    response = tokenizer.decode(out[0], skip_special_tokens=True)\n    if \"<start_of_turn>model\" in response:\n        generated = response.split(\"<start_of_turn>model\")[-1].split(\"<end_of_turn>\")[0].strip()\n    else:\n        generated = response[len(prompt):].strip()\n    scores = score_output(generated, note[\"urgency_gt\"])\n    results.append({\"note_id\": note[\"id\"], **scores})\n    print(f\"  [{i+1}/10] {note['id']} fields={scores['fields_present']}/6 urgency_match={scores['urgency_match']}\")\n\nALL_RESULTS[DISPLAY_NAME] = summarize_results(results, DISPLAY_NAME)\nfree_vram(model, tokenizer)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 10: MODEL 7 ‚Äî LoopGuard v2 (fine-tuned MedGemma)\n# Load base MedGemma + v2 LoRA adapter\n# ~10 min\n# ============================================================\nBASE_MODEL_ID = \"google/medgemma-1.5-4b-it\"\nADAPTER_DIR = \"/kaggle/working/medgemma-hypothesis-extraction-v2\"\nDISPLAY_NAME = \"LoopGuard v2\\n(fine-tuned MedGemma)\"\nprint(f\"\\nüî¨ [{7}/7] LoopGuard v2\")\nassert os.path.exists(ADAPTER_DIR), f\"‚ùå Adapter not found at {ADAPTER_DIR}. Run fine-tuning notebook first.\"\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\ntokenizer.pad_token = tokenizer.eos_token\nbase = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_ID, quantization_config=BNB_CONFIG, device_map=\"auto\",\n    trust_remote_code=True, dtype=torch.bfloat16)\nmodel = PeftModel.from_pretrained(base, ADAPTER_DIR)\nmodel.eval()\nprint(f\"‚úÖ Loaded with adapter | VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n\n# LoopGuard uses the exact prompt template from fine-tuning\nLOOPGUARD_TEMPLATE = (\n    \"<start_of_turn>user\\n\"\n    \"Extract diagnostic information from this clinical note.\\n\\n\"\n    \"Clinical Note:\\n{note}<end_of_turn>\\n\"\n    \"<start_of_turn>model\\n\"\n)\n\nresults = []\nfor i, note in enumerate(TEST_NOTES):\n    prompt = LOOPGUARD_TEMPLATE.format(note=note[\"note\"])\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=768).to(model.device)\n    with torch.no_grad():\n        out = model.generate(**inputs, max_new_tokens=400, do_sample=False,\n                             repetition_penalty=1.1, pad_token_id=tokenizer.eos_token_id)\n    response = tokenizer.decode(out[0], skip_special_tokens=True)\n    if \"<start_of_turn>model\" in response:\n        generated = response.split(\"<start_of_turn>model\")[-1].split(\"<end_of_turn>\")[0].strip()\n    else:\n        generated = response[len(prompt):].strip()\n    scores = score_output(generated, note[\"urgency_gt\"])\n    results.append({\"note_id\": note[\"id\"], **scores})\n    print(f\"  [{i+1}/10] {note['id']} fields={scores['fields_present']}/6 urgency_match={scores['urgency_match']}\")\n\nALL_RESULTS[DISPLAY_NAME] = summarize_results(results, DISPLAY_NAME)\nfree_vram(model, tokenizer)\n\nprint(f\"\\n‚úÖ All 7 models complete. {len(ALL_RESULTS)} result sets stored.\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 11: Score Table\n# ============================================================\nORDERED_MODELS = [\n    \"Gemma 2 2B\\n(general, no medical)\",\n    \"Llama 3.2 3B\\n(general, no medical)\",\n    \"BioMistral 7B\\n(medical, PubMed pretrain)\",\n    \"Qwen2.5 7B\\n(general, best <10B)\",\n    \"DeepSeek-R1 7B\\n(reasoning, no medical)\",\n    \"Base MedGemma 4B\\n(medical, zero-shot)\",\n    \"LoopGuard v2\\n(fine-tuned MedGemma)\",\n]\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"{'MODEL':<38} {'COMPLETENESS':>13} {'VALID STRUCT':>13} {'URGENCY ACC':>13}\")\nprint(\"=\" * 80)\nfor model_name in ORDERED_MODELS:\n    if model_name not in ALL_RESULTS:\n        print(f\"{model_name.replace(chr(10),' '):<38} {'NOT RUN':>13}\")\n        continue\n    s = ALL_RESULTS[model_name]\n    name = model_name.replace('\\n', ' ')\n    marker = \" ‚úÖ\" if \"LoopGuard\" in model_name else \"\"\n    print(f\"{name:<38} {s['avg_completeness']:>12.1f}% {s['pct_valid_structure']:>12.1f}% {s['pct_urgency_correct']:>12.1f}%{marker}\")\nprint(\"=\" * 80)\n\n# Save to JSON\nwith open('/kaggle/working/eval_comparison_7models.json', 'w') as f:\n    json.dump({k: {\"summary\": {\"model\": v[\"model\"], \"avg_completeness\": v[\"avg_completeness\"],\n                               \"pct_valid_structure\": v[\"pct_valid_structure\"],\n                               \"pct_urgency_correct\": v[\"pct_urgency_correct\"]},\n                   \"per_note\": v[\"per_note\"]} for k, v in ALL_RESULTS.items()}, f, indent=2)\nprint(\"\\n‚úÖ Results saved to /kaggle/working/eval_comparison_7models.json\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 12: Bar Chart\n# Saves eval_comparison.png to /kaggle/working/\n# ============================================================\nmodels_in_order = [m for m in ORDERED_MODELS if m in ALL_RESULTS]\nsummaries = [ALL_RESULTS[m] for m in models_in_order]\nlabels = [m.replace('\\n', '\\n') for m in models_in_order]\n\n# Color scheme: grays for general, blue for medical non-FT, green for LoopGuard\ncolors = []\nfor m in models_in_order:\n    if \"LoopGuard\" in m:\n        colors.append('#28a745')   # green ‚Äî winner\n    elif \"MedGemma\" in m or \"BioMistral\" in m:\n        colors.append('#4a90d9')   # blue ‚Äî medical\n    else:\n        colors.append('#9e9e9e')   # gray ‚Äî general\n\nmetrics = [\n    (\"avg_completeness\",    \"Field Completeness (%)\",  \"Out of 6 required fields\"),\n    (\"pct_valid_structure\", \"Valid Structure (%)\",      \"‚â•5/6 fields present\"),\n    (\"pct_urgency_correct\", \"Urgency Accuracy (%)\",     \"Matches ground truth label\"),\n]\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 7))\nfig.patch.set_facecolor('#f8f9fa')\n\nfor ax, (metric_key, metric_title, metric_desc) in zip(axes, metrics):\n    values = [s[metric_key] for s in summaries]\n    x = np.arange(len(labels))\n    bars = ax.bar(x, values, color=colors, width=0.65, zorder=3, edgecolor='white', linewidth=0.5)\n    ax.set_xticks(x)\n    ax.set_xticklabels(labels, fontsize=7.5)\n    ax.set_ylim(0, 115)\n    ax.set_ylabel(\"Score (%)\", fontsize=10)\n    ax.set_title(metric_title, fontsize=12, fontweight='bold', pad=10)\n    ax.set_facecolor('#ffffff')\n    ax.grid(axis='y', alpha=0.35, zorder=0)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    for bar, val in zip(bars, values):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1.5,\n                f'{val:.0f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n    ax.text(0.5, -0.18, metric_desc, transform=ax.transAxes,\n            ha='center', fontsize=8, color='#666', style='italic')\n\nlegend_items = [\n    mpatches.Patch(color='#9e9e9e', label='General model (no medical pretraining)'),\n    mpatches.Patch(color='#4a90d9', label='Medical model (literature pretrain, no task FT)'),\n    mpatches.Patch(color='#28a745', label='LoopGuard v2 (MedGemma + clinical note fine-tuning)'),\n]\nfig.legend(handles=legend_items, loc='upper center', bbox_to_anchor=(0.5, 1.04),\n           ncol=3, fontsize=9, frameon=True)\n\nfig.suptitle(\n    'LoopGuard v2 vs 6 Comparable Open-Source Models (all ‚â§7B, all locally deployable)\\n'\n    '10 Clinical Notes ¬∑ 3 Metrics ¬∑ Zero-shot except LoopGuard v2',\n    fontsize=12, fontweight='bold', y=1.10)\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/eval_comparison.png', dpi=150, bbox_inches='tight',\n            facecolor=fig.get_facecolor())\nprint(\"‚úÖ Chart saved ‚Üí /kaggle/working/eval_comparison.png\")\nprint(\"   Download this file for the competition writeup and video.\")\nplt.show()",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 13: Writeup-Ready Text\n# Copy directly into competition writeup (Technical Details section)\n# ============================================================\nprint(\"\\nüìù WRITEUP-READY TABLE (paste into Technical Details section)\")\nprint(\"=\" * 70)\n\nrows = []\nfor model_name in ORDERED_MODELS:\n    if model_name not in ALL_RESULTS:\n        continue\n    s = ALL_RESULTS[model_name]\n    name = model_name.replace('\\n', ' ')\n    rows.append(f\"| {name:<38} | {s['avg_completeness']:>11.0f}% | {s['pct_valid_structure']:>14.0f}% | {s['pct_urgency_correct']:>14.0f}% |\")\n\nheader = \"| Model                                  | Completeness | Valid Structure | Urgency Accuracy |\"\nseperator = \"|----------------------------------------|-------------|-----------------|------------------|\"\nprint(header)\nprint(seperrator := \"|\" + \"-\"*40 + \"|\" + \"-\"*13 + \"|\" + \"-\"*17 + \"|\" + \"-\"*18 + \"|\")\nfor row in rows:\n    print(row)\n\nif \"LoopGuard v2\\n(fine-tuned MedGemma)\" in ALL_RESULTS:\n    ft = ALL_RESULTS[\"LoopGuard v2\\n(fine-tuned MedGemma)\"]\n    competitors = [v for k, v in ALL_RESULTS.items() if \"LoopGuard\" not in k]\n    best_competitor_completeness = max(c['avg_completeness'] for c in competitors)\n    print(f\"\"\"\n\\nüìä KEY NUMBERS FOR WRITEUP:\n   LoopGuard v2 completeness: {ft['avg_completeness']:.0f}%\n   Best competitor completeness: {best_competitor_completeness:.0f}%\n   Gap: +{ft['avg_completeness'] - best_competitor_completeness:.0f} percentage points\n   LoopGuard valid structure rate: {ft['pct_valid_structure']:.0f}%\n   LoopGuard urgency accuracy: {ft['pct_urgency_correct']:.0f}%\n\"\"\")\n\nprint(\"=\" * 70)\nprint(\"‚úÖ Download eval_comparison.png + eval_comparison_7models.json from /kaggle/working/\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  }
 ]
}
